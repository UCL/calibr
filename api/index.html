
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation website for calibr">
      
      
        <meta name="author" content="Matt Graham">
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../LICENSE/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.19">
    
    
      
        <title>API reference - calibr</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.66ac8b77.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="calibr" class="md-header__button md-logo" aria-label="calibr" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            calibr
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API reference
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/UCL/calibr/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    UCL/calibr
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="calibr" class="md-nav__button md-logo" aria-label="calibr" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    calibr
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/UCL/calibr/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    UCL/calibr
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    API reference
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    API reference
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#calibr" class="md-nav__link">
    <span class="md-ellipsis">
      calibr
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.acquisition_functions" class="md-nav__link">
    <span class="md-ellipsis">
      acquisition_functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="acquisition_functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_expected_integrated_variance_acquisition_function" class="md-nav__link">
    <span class="md-ellipsis">
      get_expected_integrated_variance_acquisition_function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_integrated_median_interquantile_range_acquisition_function" class="md-nav__link">
    <span class="md-ellipsis">
      get_integrated_median_interquantile_range_acquisition_function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_maximum_interquantile_range_greedy_batch_acquisition_functions" class="md-nav__link">
    <span class="md-ellipsis">
      get_maximum_interquantile_range_greedy_batch_acquisition_functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_maximum_variance_greedy_batch_acquisition_functions" class="md-nav__link">
    <span class="md-ellipsis">
      get_maximum_variance_greedy_batch_acquisition_functions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.calibration" class="md-nav__link">
    <span class="md-ellipsis">
      calibration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="calibration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.calibration.calibrate" class="md-nav__link">
    <span class="md-ellipsis">
      calibrate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.calibration.get_next_inputs_batch_by_greedy_optimization" class="md-nav__link">
    <span class="md-ellipsis">
      get_next_inputs_batch_by_greedy_optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.calibration.get_next_inputs_batch_by_joint_optimization" class="md-nav__link">
    <span class="md-ellipsis">
      get_next_inputs_batch_by_joint_optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.emulation" class="md-nav__link">
    <span class="md-ellipsis">
      emulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="emulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.GaussianProcessModel" class="md-nav__link">
    <span class="md-ellipsis">
      GaussianProcessModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.fit_gaussian_process_parameters_hmc" class="md-nav__link">
    <span class="md-ellipsis">
      fit_gaussian_process_parameters_hmc
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.fit_gaussian_process_parameters_map" class="md-nav__link">
    <span class="md-ellipsis">
      fit_gaussian_process_parameters_map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.get_gaussian_process_factory" class="md-nav__link">
    <span class="md-ellipsis">
      get_gaussian_process_factory
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.optimization" class="md-nav__link">
    <span class="md-ellipsis">
      optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.ConvergenceError" class="md-nav__link">
    <span class="md-ellipsis">
      ConvergenceError
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.GlobalMinimizer" class="md-nav__link">
    <span class="md-ellipsis">
      GlobalMinimizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GlobalMinimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.GlobalMinimizer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.basin_hopping" class="md-nav__link">
    <span class="md-ellipsis">
      basin_hopping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.hessian_vector_product" class="md-nav__link">
    <span class="md-ellipsis">
      hessian_vector_product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.minimize_with_restarts" class="md-nav__link">
    <span class="md-ellipsis">
      minimize_with_restarts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../LICENSE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    License
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#calibr" class="md-nav__link">
    <span class="md-ellipsis">
      calibr
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.acquisition_functions" class="md-nav__link">
    <span class="md-ellipsis">
      acquisition_functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="acquisition_functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_expected_integrated_variance_acquisition_function" class="md-nav__link">
    <span class="md-ellipsis">
      get_expected_integrated_variance_acquisition_function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_integrated_median_interquantile_range_acquisition_function" class="md-nav__link">
    <span class="md-ellipsis">
      get_integrated_median_interquantile_range_acquisition_function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_maximum_interquantile_range_greedy_batch_acquisition_functions" class="md-nav__link">
    <span class="md-ellipsis">
      get_maximum_interquantile_range_greedy_batch_acquisition_functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.acquisition_functions.get_maximum_variance_greedy_batch_acquisition_functions" class="md-nav__link">
    <span class="md-ellipsis">
      get_maximum_variance_greedy_batch_acquisition_functions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.calibration" class="md-nav__link">
    <span class="md-ellipsis">
      calibration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="calibration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.calibration.calibrate" class="md-nav__link">
    <span class="md-ellipsis">
      calibrate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.calibration.get_next_inputs_batch_by_greedy_optimization" class="md-nav__link">
    <span class="md-ellipsis">
      get_next_inputs_batch_by_greedy_optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.calibration.get_next_inputs_batch_by_joint_optimization" class="md-nav__link">
    <span class="md-ellipsis">
      get_next_inputs_batch_by_joint_optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.emulation" class="md-nav__link">
    <span class="md-ellipsis">
      emulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="emulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.GaussianProcessModel" class="md-nav__link">
    <span class="md-ellipsis">
      GaussianProcessModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.fit_gaussian_process_parameters_hmc" class="md-nav__link">
    <span class="md-ellipsis">
      fit_gaussian_process_parameters_hmc
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.fit_gaussian_process_parameters_map" class="md-nav__link">
    <span class="md-ellipsis">
      fit_gaussian_process_parameters_map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.emulation.get_gaussian_process_factory" class="md-nav__link">
    <span class="md-ellipsis">
      get_gaussian_process_factory
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calibr.optimization" class="md-nav__link">
    <span class="md-ellipsis">
      optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.ConvergenceError" class="md-nav__link">
    <span class="md-ellipsis">
      ConvergenceError
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.GlobalMinimizer" class="md-nav__link">
    <span class="md-ellipsis">
      GlobalMinimizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GlobalMinimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.GlobalMinimizer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.basin_hopping" class="md-nav__link">
    <span class="md-ellipsis">
      basin_hopping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.hessian_vector_product" class="md-nav__link">
    <span class="md-ellipsis">
      hessian_vector_product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibr.optimization.minimize_with_restarts" class="md-nav__link">
    <span class="md-ellipsis">
      minimize_with_restarts
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/UCL/calibr/edit/main/docs/api.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


<h1 id="api-reference">API reference</h1>


<div class="doc doc-object doc-module">



<a id="calibr"></a>
  <div class="doc doc-contents first">
  
      <p>Bayesian calibration of simulations using Gaussian process emulation.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-module">



<h2 id="calibr.acquisition_functions" class="doc doc-heading">
          <span class="doc doc-object-name doc-module-name">acquisition_functions</span>


</h2>

  <div class="doc doc-contents ">
  
      <p>Acquisition functions for selecting new inputs points to evaluate model at.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="calibr.acquisition_functions.get_expected_integrated_variance_acquisition_function" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_expected_integrated_variance_acquisition_function</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_expected_integrated_variance_acquisition_function</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">,</span>
    <span class="n">integration_inputs</span><span class="p">,</span>
    <span class="n">integration_log_weights</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Construct acquisition function for minimising expected integrated variance.</p>
<p>Selects next input points to evaluate log-likelihood at which minimizes the
expectation of the integral over the input space of the variance of an unnormalized
posterior density approximation based on a Gaussian process emulator for the
log-likelihood function, with the expectation being over the posterior predictive
distribution on the unnormalized target density under the Gaussian process.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>gp_mean_and_variance</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveMeanAndVariance">PosteriorPredictiveMeanAndVariance</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating mean and variance of Gaussian process
emulator for target log-density on input space.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gp_lookahead_variance_reduction</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveLookaheadVarianceReduction">PosteriorPredictiveLookaheadVarianceReduction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating reduction in variance of
Gaussian process emulator for log-density at a test-point given one or
'pending' input points, when outputs associated with pending inputs are
assumed to follow the posterior predictive distribution under the Gaussian
process.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>integration_inputs</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Points to use when approximating integrals over input space.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>integration_log_weights</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Logarithm of weights associated with each of points
in <code>integration_inputs</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="calibr.acquisition_functions.AcquisitionFunction">AcquisitionFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The acquisition function to minimize to select new input point(s).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/acquisition_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_expected_integrated_variance_acquisition_function</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">:</span> <span class="n">PosteriorPredictiveMeanAndVariance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">:</span> <span class="n">PosteriorPredictiveLookaheadVarianceReduction</span><span class="p">,</span>
    <span class="n">integration_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="n">integration_log_weights</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AcquisitionFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct acquisition function for minimising expected integrated variance.</span>

<span class="sd">    Selects next input points to evaluate log-likelihood at which minimizes the</span>
<span class="sd">    expectation of the integral over the input space of the variance of an unnormalized</span>
<span class="sd">    posterior density approximation based on a Gaussian process emulator for the</span>
<span class="sd">    log-likelihood function, with the expectation being over the posterior predictive</span>
<span class="sd">    distribution on the unnormalized target density under the Gaussian process.</span>

<span class="sd">    Args:</span>
<span class="sd">        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process</span>
<span class="sd">            emulator for target log-density on input space.</span>
<span class="sd">        gp_lookahead_variance_reduction: Function evaluating reduction in variance of</span>
<span class="sd">            Gaussian process emulator for log-density at a test-point given one or</span>
<span class="sd">            &#39;pending&#39; input points, when outputs associated with pending inputs are</span>
<span class="sd">            assumed to follow the posterior predictive distribution under the Gaussian</span>
<span class="sd">            process.</span>
<span class="sd">        integration_inputs: Points to use when approximating integrals over input space.</span>
<span class="sd">        integration_log_weights: Logarithm of weights associated with each of points</span>
<span class="sd">            in `integration_inputs`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The acquisition function to minimize to select new input point(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mean_integration_inputs</span><span class="p">,</span> <span class="n">variance_integration_inputs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
        <span class="n">gp_mean_and_variance</span>
    <span class="p">)(</span><span class="n">integration_inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">acquisition_function</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">lookahead_variance_reduction_integration_inputs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
            <span class="n">gp_lookahead_variance_reduction</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)(</span><span class="n">integration_inputs</span><span class="p">,</span> <span class="n">new_inputs</span><span class="p">)</span>
        <span class="c1"># We neglect the initial constant wrt θ* term in</span>
        <span class="c1"># Lᵛₜ(θ*) = ∫ exp(2mₜ(θ) + s²ₜ(θ)) (exp(s²ₜ(θ)) - exp(τ²ₜ(θ; θ*))) dθ</span>
        <span class="c1"># and use</span>
        <span class="c1"># -log ∫ exp(2mₜ(θ) + s²ₜ(θ) + τ²ₜ(θ; θ*)) dθ</span>
        <span class="c1"># corresponding to the negative logarithm of the negation of the second term</span>
        <span class="c1"># in the expected integrated variance design criterion.</span>
        <span class="c1"># This appears to give a more numerically stable objective function.</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">jsp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">integration_log_weights</span>
            <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">mean_integration_inputs</span>
            <span class="o">+</span> <span class="n">variance_integration_inputs</span>
            <span class="o">+</span> <span class="n">lookahead_variance_reduction_integration_inputs</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">acquisition_function</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.acquisition_functions.get_integrated_median_interquantile_range_acquisition_function" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_integrated_median_interquantile_range_acquisition_function</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_integrated_median_interquantile_range_acquisition_function</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">,</span>
    <span class="n">integration_inputs</span><span class="p">,</span>
    <span class="n">integration_log_weights</span><span class="p">,</span>
    <span class="n">quantile_interval</span><span class="o">=</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Construct acquisition function for minimising integrated median interquantile range.</p>
<p>Selects next input points to evaluate target log-density at which minimizes the
integral over the input space of the median interquantile range of an unnormalized
target density approximation based on a Gaussian process emulator for the
log-density function, with the median being over the posterior predictive
distribution on the unnormalized target density under the Gaussian process.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>gp_mean_and_variance</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveMeanAndVariance">PosteriorPredictiveMeanAndVariance</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating mean and variance of Gaussian process
emulator for target log-density on input space.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gp_lookahead_variance_reduction</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveLookaheadVarianceReduction">PosteriorPredictiveLookaheadVarianceReduction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating reduction in variance of
Gaussian process emulator for log-density at a test-point given one or
'pending' input points, when outputs associated with pending inputs are
assumed to follow the posterior predictive distribution under the Gaussian
process.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>integration_inputs</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Points to use when approximate integrals over input space.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>integration_log_weights</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Logarithm of weights associated with each of points
in <code>integration_inputs</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>quantile_interval</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower and upper quantiles specifying inter-quantile range
to optimize.</p>
            </div>
          </td>
          <td>
                <code>(0.25, 0.75)</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="calibr.acquisition_functions.AcquisitionFunction">AcquisitionFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The acquisition function to minimize to select new input point(s).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/acquisition_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_integrated_median_interquantile_range_acquisition_function</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">:</span> <span class="n">PosteriorPredictiveMeanAndVariance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">:</span> <span class="n">PosteriorPredictiveLookaheadVarianceReduction</span><span class="p">,</span>
    <span class="n">integration_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="n">integration_log_weights</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="n">quantile_interval</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AcquisitionFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct acquisition function for minimising integrated median interquantile range.</span>

<span class="sd">    Selects next input points to evaluate target log-density at which minimizes the</span>
<span class="sd">    integral over the input space of the median interquantile range of an unnormalized</span>
<span class="sd">    target density approximation based on a Gaussian process emulator for the</span>
<span class="sd">    log-density function, with the median being over the posterior predictive</span>
<span class="sd">    distribution on the unnormalized target density under the Gaussian process.</span>

<span class="sd">    Args:</span>
<span class="sd">        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process</span>
<span class="sd">            emulator for target log-density on input space.</span>
<span class="sd">        gp_lookahead_variance_reduction: Function evaluating reduction in variance of</span>
<span class="sd">            Gaussian process emulator for log-density at a test-point given one or</span>
<span class="sd">            &#39;pending&#39; input points, when outputs associated with pending inputs are</span>
<span class="sd">            assumed to follow the posterior predictive distribution under the Gaussian</span>
<span class="sd">            process.</span>
<span class="sd">        integration_inputs: Points to use when approximate integrals over input space.</span>
<span class="sd">        integration_log_weights: Logarithm of weights associated with each of points</span>
<span class="sd">            in `integration_inputs`.</span>
<span class="sd">        quantile_interval: Lower and upper quantiles specifying inter-quantile range</span>
<span class="sd">            to optimize.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The acquisition function to minimize to select new input point(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">jsp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">ndtri</span><span class="p">(</span><span class="n">quantile_interval</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">jsp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">ndtri</span><span class="p">(</span><span class="n">quantile_interval</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">mean_integration_inputs</span><span class="p">,</span> <span class="n">variance_integration_inputs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
        <span class="n">gp_mean_and_variance</span>
    <span class="p">)(</span><span class="n">integration_inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">acquisition_function</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">lookahead_variance_reduction_integration_inputs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span>
            <span class="n">gp_lookahead_variance_reduction</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)(</span><span class="n">integration_inputs</span><span class="p">,</span> <span class="n">new_inputs</span><span class="p">)</span>
        <span class="n">lookahead_standard_deviation_integration_inputs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">abs</span><span class="p">(</span>
                <span class="n">variance_integration_inputs</span>
                <span class="o">-</span> <span class="n">lookahead_variance_reduction_integration_inputs</span>
            <span class="p">)</span>
            <span class="o">**</span> <span class="mf">0.5</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">jsp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">integration_log_weights</span>
            <span class="o">+</span> <span class="n">mean_integration_inputs</span>
            <span class="o">+</span> <span class="n">upper</span> <span class="o">*</span> <span class="n">lookahead_standard_deviation_integration_inputs</span>
            <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span>
                <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">lower</span> <span class="o">-</span> <span class="n">upper</span><span class="p">)</span> <span class="o">*</span> <span class="n">lookahead_standard_deviation_integration_inputs</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">acquisition_function</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.acquisition_functions.get_maximum_interquantile_range_greedy_batch_acquisition_functions" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_maximum_interquantile_range_greedy_batch_acquisition_functions</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_maximum_interquantile_range_greedy_batch_acquisition_functions</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">,</span>
    <span class="n">quantile_interval</span><span class="o">=</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Construct acquisition function for greedy maximisation of interquantile range.</p>
<p>Selects next input points to evaluate target log-density at which maximise
interquantile range of an unnormalized target density approximation based on a
Gaussian process emulator for the log-density function. After an initial point is
chosen, subsequent points in the batch are selected in a greedy fashion by
maximising the interquantile range given the already selected point(s), assuming the
log-density values at the already selected points are distributed according the
Gaussian process predictive distribution.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>gp_mean_and_variance</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveMeanAndVariance">PosteriorPredictiveMeanAndVariance</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating mean and variance of Gaussian process
emulator for target log-density on input space.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gp_lookahead_variance_reduction</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveLookaheadVarianceReduction">PosteriorPredictiveLookaheadVarianceReduction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating reduction in variance of
Gaussian process emulator for log-density at a test-point given one or
'pending' input points, when outputs associated with pending inputs are
assumed to follow the posterior predictive distribution under the Gaussian
process.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>quantile_interval</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower and upper quantiles specifying inter-quantile range
to optimize.</p>
            </div>
          </td>
          <td>
                <code>(0.25, 0.75)</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The acquisition function to minimize to select new input point(s). The function</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>takes one or two arguments. If a single argument is passed it corresponds to</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the acquisition function for the initial point in a batch. If two arguments are</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>passed, the first argument corresponds to the new input point being chosen and</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the second argument to the already selected input point(s) in the batch.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/acquisition_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_maximum_interquantile_range_greedy_batch_acquisition_functions</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">:</span> <span class="n">PosteriorPredictiveMeanAndVariance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">:</span> <span class="n">PosteriorPredictiveLookaheadVarianceReduction</span><span class="p">,</span>
    <span class="n">quantile_interval</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct acquisition function for greedy maximisation of interquantile range.</span>

<span class="sd">    Selects next input points to evaluate target log-density at which maximise</span>
<span class="sd">    interquantile range of an unnormalized target density approximation based on a</span>
<span class="sd">    Gaussian process emulator for the log-density function. After an initial point is</span>
<span class="sd">    chosen, subsequent points in the batch are selected in a greedy fashion by</span>
<span class="sd">    maximising the interquantile range given the already selected point(s), assuming the</span>
<span class="sd">    log-density values at the already selected points are distributed according the</span>
<span class="sd">    Gaussian process predictive distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process</span>
<span class="sd">            emulator for target log-density on input space.</span>
<span class="sd">        gp_lookahead_variance_reduction: Function evaluating reduction in variance of</span>
<span class="sd">            Gaussian process emulator for log-density at a test-point given one or</span>
<span class="sd">            &#39;pending&#39; input points, when outputs associated with pending inputs are</span>
<span class="sd">            assumed to follow the posterior predictive distribution under the Gaussian</span>
<span class="sd">            process.</span>
<span class="sd">        quantile_interval: Lower and upper quantiles specifying inter-quantile range</span>
<span class="sd">            to optimize.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The acquisition function to minimize to select new input point(s). The function</span>
<span class="sd">        takes one or two arguments. If a single argument is passed it corresponds to</span>
<span class="sd">        the acquisition function for the initial point in a batch. If two arguments are</span>
<span class="sd">        passed, the first argument corresponds to the new input point being chosen and</span>
<span class="sd">        the second argument to the already selected input point(s) in the batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">jsp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">ndtri</span><span class="p">(</span><span class="n">quantile_interval</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">jsp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">ndtri</span><span class="p">(</span><span class="n">quantile_interval</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">acquisition_function</span><span class="p">(</span>
        <span class="n">new_input</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">pending_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">gp_mean_and_variance</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pending_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lookahead_variance_reduction</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lookahead_variance_reduction</span> <span class="o">=</span> <span class="n">gp_lookahead_variance_reduction</span><span class="p">(</span>
                <span class="n">new_input</span><span class="p">,</span> <span class="n">pending_inputs</span>
            <span class="p">)</span>
        <span class="n">lookahead_standard_deviation</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">abs</span><span class="p">(</span><span class="n">variance</span> <span class="o">-</span> <span class="n">lookahead_variance_reduction</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="o">-</span><span class="n">mean</span>
            <span class="o">-</span> <span class="n">upper</span> <span class="o">*</span> <span class="n">lookahead_standard_deviation</span>
            <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lookahead_standard_deviation</span> <span class="o">*</span> <span class="p">(</span><span class="n">lower</span> <span class="o">-</span> <span class="n">upper</span><span class="p">)))</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">acquisition_function</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.acquisition_functions.get_maximum_variance_greedy_batch_acquisition_functions" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_maximum_variance_greedy_batch_acquisition_functions</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_maximum_variance_greedy_batch_acquisition_functions</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">,</span> <span class="n">gp_lookahead_variance_reduction</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Construct acquisition functions for greedy maximisation of variance.</p>
<p>Selects next input points to evaluate target log-density at which maximise variance
of an unnormalized target density approximation based on a Gaussian process emulator
for the log-density function. After an initial point is chosen, subsequent points in
the batch are selected in a greedy fashion by maximising the variance given the
already selected point(s), assuming the log-density values at the already selected
points are distributed according the Gaussian process predictive distribution.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>gp_mean_and_variance</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveMeanAndVariance">PosteriorPredictiveMeanAndVariance</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating mean and variance of Gaussian process
emulator for target log-density on input space.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gp_lookahead_variance_reduction</code></td>
          <td>
                <code><span title="emul.types.PosteriorPredictiveLookaheadVarianceReduction">PosteriorPredictiveLookaheadVarianceReduction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function evaluating reduction in variance of
Gaussian process emulator for log-density at a test-point given one or
'pending' input points, when outputs associated with pending inputs are
assumed to follow the posterior predictive distribution under the Gaussian
process.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The acquisition function to minimize to select new input point(s). The function</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>takes one or two arguments. If a single argument is passed it corresponds to</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the acquisition function for the initial point in a batch. If two arguments are</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>passed, the first argument corresponds to the new input point being chosen and</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the second argument to the already selected input point(s) in the batch.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/acquisition_functions.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_maximum_variance_greedy_batch_acquisition_functions</span><span class="p">(</span>
    <span class="n">gp_mean_and_variance</span><span class="p">:</span> <span class="n">PosteriorPredictiveMeanAndVariance</span><span class="p">,</span>
    <span class="n">gp_lookahead_variance_reduction</span><span class="p">:</span> <span class="n">PosteriorPredictiveLookaheadVarianceReduction</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct acquisition functions for greedy maximisation of variance.</span>

<span class="sd">    Selects next input points to evaluate target log-density at which maximise variance</span>
<span class="sd">    of an unnormalized target density approximation based on a Gaussian process emulator</span>
<span class="sd">    for the log-density function. After an initial point is chosen, subsequent points in</span>
<span class="sd">    the batch are selected in a greedy fashion by maximising the variance given the</span>
<span class="sd">    already selected point(s), assuming the log-density values at the already selected</span>
<span class="sd">    points are distributed according the Gaussian process predictive distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process</span>
<span class="sd">            emulator for target log-density on input space.</span>
<span class="sd">        gp_lookahead_variance_reduction: Function evaluating reduction in variance of</span>
<span class="sd">            Gaussian process emulator for log-density at a test-point given one or</span>
<span class="sd">            &#39;pending&#39; input points, when outputs associated with pending inputs are</span>
<span class="sd">            assumed to follow the posterior predictive distribution under the Gaussian</span>
<span class="sd">            process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The acquisition function to minimize to select new input point(s). The function</span>
<span class="sd">        takes one or two arguments. If a single argument is passed it corresponds to</span>
<span class="sd">        the acquisition function for the initial point in a batch. If two arguments are</span>
<span class="sd">        passed, the first argument corresponds to the new input point being chosen and</span>
<span class="sd">        the second argument to the already selected input point(s) in the batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">acquisition_function</span><span class="p">(</span>
        <span class="n">new_input</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">pending_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">gp_mean_and_variance</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pending_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lookahead_variance_reduction</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lookahead_variance_reduction</span> <span class="o">=</span> <span class="n">gp_lookahead_variance_reduction</span><span class="p">(</span>
                <span class="n">new_input</span><span class="p">,</span> <span class="n">pending_inputs</span>
            <span class="p">)</span>
        <span class="n">lookahead_variance</span> <span class="o">=</span> <span class="n">variance</span> <span class="o">-</span> <span class="n">lookahead_variance_reduction</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">variance</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lookahead_variance</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">acquisition_function</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="calibr.calibration" class="doc doc-heading">
          <span class="doc doc-object-name doc-module-name">calibration</span>


</h2>

  <div class="doc doc-contents ">
  
      <p>Functions for iteratively calibrating the parameters of a probabilistic model.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="calibr.calibration.calibrate" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">calibrate</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">calibrate</span><span class="p">(</span>
    <span class="n">num_initial_inputs</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="n">sample_initial_inputs</span><span class="p">,</span>
    <span class="n">posterior_log_density_batch</span><span class="p">,</span>
    <span class="n">gaussian_process_factory</span><span class="p">,</span>
    <span class="n">get_integration_points_and_log_weights</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fit_gaussian_process_parameters</span><span class="o">=</span><span class="n">fit_gaussian_process_parameters_map</span><span class="p">,</span>
    <span class="n">get_acquisition_function</span><span class="o">=</span><span class="n">get_integrated_median_interquantile_range_acquisition_function</span><span class="p">,</span>
    <span class="n">get_next_inputs_batch</span><span class="o">=</span><span class="n">get_next_inputs_batch_by_joint_optimization</span><span class="p">,</span>
    <span class="n">end_of_iteration_callback</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Estimate the posterior on the unknown inputs of an (expensive to evaluate) model.</p>
<p>Iterates evaluating log density for posterior at a batch of inputs (unknown
variables to infer posterior on), fitting a Gaussian process to the log density
evaluations so far and optimizing an acquisition function using the Gaussian process
emulator to choose a new batch of input points at which to evaluate the log density
(by minimizing a measure of the expected uncertainty in the emulator about the log
posterior density function).</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>num_initial_inputs</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of initial inputs to evaluate posterior log density
at to initialize calibration.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>batch_size</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Size of batch of inputs to optimize for and evaluate log density at
in each calibration iteration.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>num_iterations</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of calibration iterations to perform. Total number of
model posterior log density evaluations is
<code>num_initial_inputs + batch_size * num_iterations</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Seeded NumPy random number generator.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_initial_inputs</code></td>
          <td>
                <code><span title="calibr.calibration.InitialInputSampler">InitialInputSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function outputting reasonable random initial values for
batch of inputs when passed a random number generator and batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>posterior_log_density_batch</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a>], <a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function computing logarithm of (unnormalized)
posterior density on model inputs, for a batch of inputs (with passed
argument being a two dimensional array with first dimension the batch
index).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gaussian_process_factory</code></td>
          <td>
                <code><span title="calibr.emulation.GaussianProcessFactory">GaussianProcessFactory</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Factory function generating Gaussian process models
given a data dictionary.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>get_integration_points_and_log_weights</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a>, <span title="calibr.calibration.InitialInputSampler">InitialInputSampler</span>, <span title="emul.types.PosteriorPredictiveMeanAndVariance">PosteriorPredictiveMeanAndVariance</span>], <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>, <a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function which outputs points in input
space and corresponding (log) weights by which to estimate integrals over
the input space in the acquisition function as a weighted sum. The function
is passed a seeded random number generator, a function to sample random
points in the input space and the current Gaussian process posterior
predictive mean and variance function. The input points and weights may for
example be generated according to a (deterministic) numerical quadrature
rule for low dimensionalities, a stochastic (quasi-) Monte Carlo scheme for
moderate dimensionalities or a Markov chain Monte Carlo or sequential Monte
Carlo scheme for higher dimensionalities.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>fit_gaussian_process_parameters</code></td>
          <td>
                <code><span title="calibr.emulation.GaussianProcessParameterFitter">GaussianProcessParameterFitter</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function which fits the parameters of a
Gaussian process model given the current data (input-output pairs). Passed
a seeded random number generator and tuple of Gaussian process model
functions.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.emulation.fit_gaussian_process_parameters_map" href="#calibr.emulation.fit_gaussian_process_parameters_map">fit_gaussian_process_parameters_map</a></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>get_acquisition_function</code></td>
          <td>
                <code><span title="calibr.acquisition_functions.AcquisitionFunctionFactory">AcquisitionFunctionFactory</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Factory function generating acquisition functions
given Gaussian process posterior predictive functions.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.acquisition_functions.get_integrated_median_interquantile_range_acquisition_function" href="#calibr.acquisition_functions.get_integrated_median_interquantile_range_acquisition_function">get_integrated_median_interquantile_range_acquisition_function</a></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>get_next_inputs_batch</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a>, <span title="calibr.acquisition_functions.AcquisitionFunction">AcquisitionFunction</span>, <span title="calibr.calibration.InitialInputSampler">InitialInputSampler</span>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a>], <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function which computes next batch of inputs to evaluate
model at by optimizing the current acquisition function. Passed a seeded
random number generator, acquisition function, input sampler and batch size.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.calibration.get_next_inputs_batch_by_joint_optimization" href="#calibr.calibration.get_next_inputs_batch_by_joint_optimization">get_next_inputs_batch_by_joint_optimization</a></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>end_of_iteration_callback</code></td>
          <td>
                <code><span title="calibr.calibration.EndOfIterationCallback">EndOfIterationCallback</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optional callback function evaluate at end of each
calibration iteration, for example for logging metrics or plotting / saving
intermediate outputs. Passed current iteration index, Gaussian process
posterior mean and variance, data dictionary with all inputs and
corresponding log density evaluations so far, batch of inputs selected in
current iteration and corresponding optimized acquisition function value.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.emulation.GaussianProcessModel" href="#calibr.emulation.GaussianProcessModel">GaussianProcessModel</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of Gaussian process model, data dictionary containing all model inputs</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><span title="emul.types.DataDict">DataDict</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>and log density evaluations and fitted Gaussian process model parameters at</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><span title="emul.types.ParametersDict">ParametersDict</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>final iteration.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/calibration.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">calibrate</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913</span>
    <span class="n">num_initial_inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="n">sample_initial_inputs</span><span class="p">:</span> <span class="n">InitialInputSampler</span><span class="p">,</span>
    <span class="n">posterior_log_density_batch</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">ArrayLike</span><span class="p">],</span> <span class="n">Array</span><span class="p">],</span>
    <span class="n">gaussian_process_factory</span><span class="p">:</span> <span class="n">GaussianProcessFactory</span><span class="p">,</span>
    <span class="n">get_integration_points_and_log_weights</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span><span class="n">Generator</span><span class="p">,</span> <span class="n">InitialInputSampler</span><span class="p">,</span> <span class="n">PosteriorPredictiveMeanAndVariance</span><span class="p">],</span>
        <span class="nb">tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">],</span>
    <span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fit_gaussian_process_parameters</span><span class="p">:</span> <span class="n">GaussianProcessParameterFitter</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">fit_gaussian_process_parameters_map</span>
    <span class="p">),</span>
    <span class="n">get_acquisition_function</span><span class="p">:</span> <span class="n">AcquisitionFunctionFactory</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">get_integrated_median_interquantile_range_acquisition_function</span>
    <span class="p">),</span>
    <span class="n">get_next_inputs_batch</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span><span class="n">Generator</span><span class="p">,</span> <span class="n">AcquisitionFunction</span><span class="p">,</span> <span class="n">InitialInputSampler</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">get_next_inputs_batch_by_joint_optimization</span><span class="p">,</span>
    <span class="n">end_of_iteration_callback</span><span class="p">:</span> <span class="n">EndOfIterationCallback</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">GaussianProcessModel</span><span class="p">,</span> <span class="n">DataDict</span><span class="p">,</span> <span class="n">ParametersDict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate the posterior on the unknown inputs of an (expensive to evaluate) model.</span>

<span class="sd">    Iterates evaluating log density for posterior at a batch of inputs (unknown</span>
<span class="sd">    variables to infer posterior on), fitting a Gaussian process to the log density</span>
<span class="sd">    evaluations so far and optimizing an acquisition function using the Gaussian process</span>
<span class="sd">    emulator to choose a new batch of input points at which to evaluate the log density</span>
<span class="sd">    (by minimizing a measure of the expected uncertainty in the emulator about the log</span>
<span class="sd">    posterior density function).</span>

<span class="sd">    Args:</span>
<span class="sd">        num_initial_inputs: Number of initial inputs to evaluate posterior log density</span>
<span class="sd">            at to initialize calibration.</span>
<span class="sd">        batch_size: Size of batch of inputs to optimize for and evaluate log density at</span>
<span class="sd">            in each calibration iteration.</span>
<span class="sd">        num_iterations: Number of calibration iterations to perform. Total number of</span>
<span class="sd">            model posterior log density evaluations is</span>
<span class="sd">            `num_initial_inputs + batch_size * num_iterations`.</span>
<span class="sd">        rng: Seeded NumPy random number generator.</span>
<span class="sd">        sample_initial_inputs: Function outputting reasonable random initial values for</span>
<span class="sd">            batch of inputs when passed a random number generator and batch size.</span>
<span class="sd">        posterior_log_density_batch: Function computing logarithm of (unnormalized)</span>
<span class="sd">            posterior density on model inputs, for a batch of inputs (with passed</span>
<span class="sd">            argument being a two dimensional array with first dimension the batch</span>
<span class="sd">            index).</span>
<span class="sd">        gaussian_process_factory: Factory function generating Gaussian process models</span>
<span class="sd">            given a data dictionary.</span>
<span class="sd">        get_integration_points_and_log_weights: Function which outputs points in input</span>
<span class="sd">            space and corresponding (log) weights by which to estimate integrals over</span>
<span class="sd">            the input space in the acquisition function as a weighted sum. The function</span>
<span class="sd">            is passed a seeded random number generator, a function to sample random</span>
<span class="sd">            points in the input space and the current Gaussian process posterior</span>
<span class="sd">            predictive mean and variance function. The input points and weights may for</span>
<span class="sd">            example be generated according to a (deterministic) numerical quadrature</span>
<span class="sd">            rule for low dimensionalities, a stochastic (quasi-) Monte Carlo scheme for</span>
<span class="sd">            moderate dimensionalities or a Markov chain Monte Carlo or sequential Monte</span>
<span class="sd">            Carlo scheme for higher dimensionalities.</span>
<span class="sd">        fit_gaussian_process_parameters: Function which fits the parameters of a</span>
<span class="sd">            Gaussian process model given the current data (input-output pairs). Passed</span>
<span class="sd">            a seeded random number generator and tuple of Gaussian process model</span>
<span class="sd">            functions.</span>
<span class="sd">        get_acquisition_function: Factory function generating acquisition functions</span>
<span class="sd">            given Gaussian process posterior predictive functions.</span>
<span class="sd">        get_next_inputs_batch: Function which computes next batch of inputs to evaluate</span>
<span class="sd">            model at by optimizing the current acquisition function. Passed a seeded</span>
<span class="sd">            random number generator, acquisition function, input sampler and batch size.</span>
<span class="sd">        end_of_iteration_callback: Optional callback function evaluate at end of each</span>
<span class="sd">            calibration iteration, for example for logging metrics or plotting / saving</span>
<span class="sd">            intermediate outputs. Passed current iteration index, Gaussian process</span>
<span class="sd">            posterior mean and variance, data dictionary with all inputs and</span>
<span class="sd">            corresponding log density evaluations so far, batch of inputs selected in</span>
<span class="sd">            current iteration and corresponding optimized acquisition function value.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of Gaussian process model, data dictionary containing all model inputs</span>
<span class="sd">        and log density evaluations and fitted Gaussian process model parameters at</span>
<span class="sd">        final iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">sample_initial_inputs</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">num_initial_inputs</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">:</span> <span class="n">posterior_log_density_batch</span><span class="p">(</span><span class="n">inputs</span><span class="p">)}</span>
    <span class="k">for</span> <span class="n">iteration_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">gaussian_process</span> <span class="o">=</span> <span class="n">gaussian_process_factory</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">fit_gaussian_process_parameters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">gaussian_process</span><span class="p">)</span>
        <span class="p">(</span>
            <span class="n">posterior_mean_and_variance</span><span class="p">,</span>
            <span class="n">lookahead_variance_reduction</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">gaussian_process</span><span class="o">.</span><span class="n">get_posterior_functions</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">iteration_index</span> <span class="o">==</span> <span class="n">num_iterations</span><span class="p">:</span>
            <span class="c1"># In final iteration, only fit Gaussian process to all model evaluations</span>
            <span class="c1"># computed so far without acquiring a new set of inputs to evaluate</span>
            <span class="k">break</span>
        <span class="p">(</span>
            <span class="n">integration_inputs</span><span class="p">,</span>
            <span class="n">integration_log_weights</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">get_integration_points_and_log_weights</span><span class="p">(</span>
            <span class="n">rng</span><span class="p">,</span> <span class="n">sample_initial_inputs</span><span class="p">,</span> <span class="n">posterior_mean_and_variance</span>
        <span class="p">)</span>
        <span class="n">acquisition_function</span> <span class="o">=</span> <span class="n">get_acquisition_function</span><span class="p">(</span>
            <span class="n">posterior_mean_and_variance</span><span class="p">,</span>
            <span class="n">lookahead_variance_reduction</span><span class="p">,</span>
            <span class="n">integration_inputs</span><span class="p">,</span>
            <span class="n">integration_log_weights</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">next_inputs</span><span class="p">,</span> <span class="n">acquisition_function_value</span> <span class="o">=</span> <span class="n">get_next_inputs_batch</span><span class="p">(</span>
            <span class="n">rng</span><span class="p">,</span> <span class="n">acquisition_function</span><span class="p">,</span> <span class="n">sample_initial_inputs</span><span class="p">,</span> <span class="n">batch_size</span>
        <span class="p">)</span>
        <span class="n">next_outputs</span> <span class="o">=</span> <span class="n">posterior_log_density_batch</span><span class="p">(</span><span class="n">next_inputs</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">next_inputs</span><span class="p">)),</span>
            <span class="s2">&quot;outputs&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;outputs&quot;</span><span class="p">],</span> <span class="n">next_outputs</span><span class="p">)),</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">end_of_iteration_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">end_of_iteration_callback</span><span class="p">(</span>
                <span class="n">iteration_index</span><span class="p">,</span>
                <span class="n">posterior_mean_and_variance</span><span class="p">,</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="n">next_inputs</span><span class="p">,</span>
                <span class="n">acquisition_function_value</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">gaussian_process</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">parameters</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.calibration.get_next_inputs_batch_by_greedy_optimization" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_next_inputs_batch_by_greedy_optimization</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_next_inputs_batch_by_greedy_optimization</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="n">acquisition_function</span><span class="p">,</span>
    <span class="n">sample_initial_inputs</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">minimize_function</span><span class="o">=</span><span class="n">minimize_with_restarts</span><span class="p">,</span>
    <span class="o">**</span><span class="n">minimize_function_kwargs</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Get next batch of inputs to evaluate by greedily optimizing acquisition function.</p>
<p>Sequentially minimizes acquisition function for <code>b</code> in 1 to <code>batch_size</code> by fixing
<code>b - 1</code> inputs already optimized and minimizing over a single new input in each
iteration.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>NumPy random number generator for initializing optimization runs.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>acquisition_function</code></td>
          <td>
                <code><span title="calibr.acquisition_functions.AcquisitionFunction">AcquisitionFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Scalar-valued function of a batch of inputs to optimize to
find new batch of inputs to evaluate model for.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_initial_inputs</code></td>
          <td>
                <code><span title="calibr.calibration.InitialInputSampler">InitialInputSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function outputting reasonable random initial values for
batch of inputs when passed a random number generator and batch size. Used
to initialize state for optimization runs.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>batch_size</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of inputs in batch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_function</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.optimization.GlobalMinimizer" href="#calibr.optimization.GlobalMinimizer">GlobalMinimizer</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function used to attempt to find minimum of (sequence of)
acquisition functions.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.optimization.minimize_with_restarts" href="#calibr.optimization.minimize_with_restarts">minimize_with_restarts</a></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>**minimize_function_kwargs</code></td>
          <td>
                <code><span title="calibr.optimization.GlobalMinimizerKwarg">GlobalMinimizerKwarg</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Any keyword arguments to pass to
<code>minimize_function</code> function used to optimize acquisition function.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of optimized inputs batch and corresponding value of acquisition function.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/calibration.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_next_inputs_batch_by_greedy_optimization</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="n">acquisition_function</span><span class="p">:</span> <span class="n">AcquisitionFunction</span><span class="p">,</span>
    <span class="n">sample_initial_inputs</span><span class="p">:</span> <span class="n">InitialInputSampler</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">minimize_function</span><span class="p">:</span> <span class="n">GlobalMinimizer</span> <span class="o">=</span> <span class="n">minimize_with_restarts</span><span class="p">,</span>
    <span class="o">**</span><span class="n">minimize_function_kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get next batch of inputs to evaluate by greedily optimizing acquisition function.</span>

<span class="sd">    Sequentially minimizes acquisition function for `b` in 1 to `batch_size` by fixing</span>
<span class="sd">    `b - 1` inputs already optimized and minimizing over a single new input in each</span>
<span class="sd">    iteration.</span>

<span class="sd">    Args:</span>
<span class="sd">        rng: NumPy random number generator for initializing optimization runs.</span>
<span class="sd">        acquisition_function: Scalar-valued function of a batch of inputs to optimize to</span>
<span class="sd">            find new batch of inputs to evaluate model for.</span>
<span class="sd">        sample_initial_inputs: Function outputting reasonable random initial values for</span>
<span class="sd">            batch of inputs when passed a random number generator and batch size. Used</span>
<span class="sd">            to initialize state for optimization runs.</span>
<span class="sd">        batch_size: Number of inputs in batch.</span>
<span class="sd">        minimize_function: Function used to attempt to find minimum of (sequence of)</span>
<span class="sd">            acquisition functions.</span>
<span class="sd">        **minimize_function_kwargs: Any keyword arguments to pass to</span>
<span class="sd">            `minimize_function` function used to optimize acquisition function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of optimized inputs batch and corresponding value of acquisition function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">acquisition_function_greedy</span><span class="p">(</span>
        <span class="n">current_input</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ArrayLike</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">acquisition_function</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">current_input</span><span class="p">,</span> <span class="o">*</span><span class="n">fixed_inputs</span><span class="p">]))</span>

    <span class="n">fixed_inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ArrayLike</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">current_input</span><span class="p">,</span> <span class="n">min_acquisition_function</span> <span class="o">=</span> <span class="n">minimize_function</span><span class="p">(</span>
            <span class="n">objective_function</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span>
                <span class="n">acquisition_function_greedy</span><span class="p">,</span> <span class="n">fixed_inputs</span><span class="o">=</span><span class="n">fixed_inputs</span>
            <span class="p">),</span>
            <span class="n">sample_initial_state</span><span class="o">=</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">sample_initial_inputs</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
            <span class="o">**</span><span class="n">minimize_function_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">fixed_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_input</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">fixed_inputs</span><span class="p">),</span> <span class="n">min_acquisition_function</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.calibration.get_next_inputs_batch_by_joint_optimization" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_next_inputs_batch_by_joint_optimization</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_next_inputs_batch_by_joint_optimization</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="n">acquisition_function</span><span class="p">,</span>
    <span class="n">sample_initial_inputs</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">minimize_function</span><span class="o">=</span><span class="n">minimize_with_restarts</span><span class="p">,</span>
    <span class="o">**</span><span class="n">minimize_function_kwargs</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Get next batch of inputs to evaluate by jointly optimizing acquisition function.</p>
<p>Minimizes acquisition function over product of <code>batch_size</code> input spaces.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>NumPy random number generator for initializing optimization runs.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>acquisition_function</code></td>
          <td>
                <code><span title="calibr.acquisition_functions.AcquisitionFunction">AcquisitionFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Scalar-valued function of a batch of inputs to optimize to
find new batch of inputs to evaluate model for.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_initial_inputs</code></td>
          <td>
                <code><span title="calibr.calibration.InitialInputSampler">InitialInputSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function outputting reasonable random initial values for
batch of inputs when passed a random number generator and batch size. Used
to initialize state for optimization runs.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>batch_size</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of inputs in batch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_function</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.optimization.GlobalMinimizer" href="#calibr.optimization.GlobalMinimizer">GlobalMinimizer</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function used to attempt to find minimum of acquisition
function.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.optimization.minimize_with_restarts" href="#calibr.optimization.minimize_with_restarts">minimize_with_restarts</a></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>**minimize_function_kwargs</code></td>
          <td>
                <code><span title="calibr.optimization.GlobalMinimizerKwarg">GlobalMinimizerKwarg</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Any keyword arguments to pass to
<code>minimize_function</code> function used to optimize acquisition function.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>, <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of optimized inputs batch and corresponding value of acquisition function.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/calibration.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_next_inputs_batch_by_joint_optimization</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="n">acquisition_function</span><span class="p">:</span> <span class="n">AcquisitionFunction</span><span class="p">,</span>
    <span class="n">sample_initial_inputs</span><span class="p">:</span> <span class="n">InitialInputSampler</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">minimize_function</span><span class="p">:</span> <span class="n">GlobalMinimizer</span> <span class="o">=</span> <span class="n">minimize_with_restarts</span><span class="p">,</span>
    <span class="o">**</span><span class="n">minimize_function_kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get next batch of inputs to evaluate by jointly optimizing acquisition function.</span>

<span class="sd">    Minimizes acquisition function over product of `batch_size` input spaces.</span>

<span class="sd">    Args:</span>
<span class="sd">        rng: NumPy random number generator for initializing optimization runs.</span>
<span class="sd">        acquisition_function: Scalar-valued function of a batch of inputs to optimize to</span>
<span class="sd">            find new batch of inputs to evaluate model for.</span>
<span class="sd">        sample_initial_inputs: Function outputting reasonable random initial values for</span>
<span class="sd">            batch of inputs when passed a random number generator and batch size. Used</span>
<span class="sd">            to initialize state for optimization runs.</span>
<span class="sd">        batch_size: Number of inputs in batch.</span>
<span class="sd">        minimize_function: Function used to attempt to find minimum of acquisition</span>
<span class="sd">            function.</span>
<span class="sd">        **minimize_function_kwargs: Any keyword arguments to pass to</span>
<span class="sd">            `minimize_function` function used to optimize acquisition function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of optimized inputs batch and corresponding value of acquisition function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">acquisition_function_flat_input</span><span class="p">(</span><span class="n">flat_inputs</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">acquisition_function</span><span class="p">(</span><span class="n">flat_inputs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">minimize_function</span> <span class="ow">is</span> <span class="n">minimize_with_restarts</span><span class="p">:</span>
        <span class="n">minimize_function_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;number_minima_to_find&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">minimize_function_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;maximum_minimize_calls&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">minimize_function_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;minimize_method&quot;</span><span class="p">,</span> <span class="s2">&quot;Newton-CG&quot;</span><span class="p">)</span>

    <span class="n">flat_inputs</span><span class="p">,</span> <span class="n">min_acquisition_function</span> <span class="o">=</span> <span class="n">minimize_function</span><span class="p">(</span>
        <span class="n">objective_function</span><span class="o">=</span><span class="n">acquisition_function_flat_input</span><span class="p">,</span>
        <span class="n">sample_initial_state</span><span class="o">=</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">sample_initial_inputs</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
        <span class="o">**</span><span class="n">minimize_function_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">flat_inputs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">min_acquisition_function</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="calibr.emulation" class="doc doc-heading">
          <span class="doc doc-object-name doc-module-name">emulation</span>


</h2>

  <div class="doc doc-contents ">
  
      <p>Functions and types for constructing and fitting Gaussian process emulators.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="calibr.emulation.GaussianProcessModel" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">GaussianProcessModel</span>


</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-external" title="typing.NamedTuple" href="https://docs.python.org/3/library/typing.html#typing.NamedTuple">NamedTuple</a></code></p>

  
      <p>Wrapper for functions associated with a Gaussian process model.</p>

            <details class="quote">
              <summary>Source code in <code>src/calibr/emulation.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">GaussianProcessModel</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper for functions associated with a Gaussian process model.&quot;&quot;&quot;</span>

    <span class="n">neg_log_marginal_posterior</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">ArrayLike</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span>
    <span class="n">get_posterior_functions</span><span class="p">:</span> <span class="n">PosteriorPredictiveFunctionFactory</span>
    <span class="n">transform_parameters</span><span class="p">:</span> <span class="n">ParameterTransformer</span>
    <span class="n">sample_unconstrained_parameters</span><span class="p">:</span> <span class="n">UnconstrainedParametersSampler</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>



<div class="doc doc-object doc-function">



<h3 id="calibr.emulation.fit_gaussian_process_parameters_hmc" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">fit_gaussian_process_parameters_hmc</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_gaussian_process_parameters_hmc</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="n">gaussian_process</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">n_chain</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_warm_up_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">n_main_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">r_hat_threshold</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Fit parameters of Gaussian process model by sampling posterior using HMC.</p>
<p>Uses Hamiltonian Monte Carlo (HMC) to generate chain(s) of samples approximating
posterior distribution on Gaussian process parameters given data.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Seeded NumPy random number generator.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gaussian_process</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.emulation.GaussianProcessModel" href="#calibr.emulation.GaussianProcessModel">GaussianProcessModel</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of functions for Gaussian process model to fit.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>n_chain</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of Markov chains to simulate.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>n_warm_up_iter</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of adaptive warm-up iterations to run for each chain.</p>
            </div>
          </td>
          <td>
                <code>500</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>n_main_iter</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of main sampling stage iterations to run for each chain.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>r_hat_threshold</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not <code>None</code>, specifies a maximum value for the
(rank-normalized, split) R-hat convergence diagnostic computed from the
chains, with R-hat values exceeding this threshold leading to an
exception being raised. Requires <code>n_chain &gt; 1</code> and for ArviZ package to
be installed.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="emul.types.ParametersDict">ParametersDict</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dictionary of parameters corresponding to approximate posterior sample.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/emulation.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_gaussian_process_parameters_hmc</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="n">gaussian_process</span><span class="p">:</span> <span class="n">GaussianProcessModel</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">n_chain</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_warm_up_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">n_main_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">r_hat_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParametersDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit parameters of Gaussian process model by sampling posterior using HMC.</span>

<span class="sd">    Uses Hamiltonian Monte Carlo (HMC) to generate chain(s) of samples approximating</span>
<span class="sd">    posterior distribution on Gaussian process parameters given data.</span>

<span class="sd">    Args:</span>
<span class="sd">        rng: Seeded NumPy random number generator.</span>
<span class="sd">        gaussian_process: Tuple of functions for Gaussian process model to fit.</span>
<span class="sd">        n_chain: Number of Markov chains to simulate.</span>
<span class="sd">        n_warm_up_iter: Number of adaptive warm-up iterations to run for each chain.</span>
<span class="sd">        n_main_iter: Number of main sampling stage iterations to run for each chain.</span>
<span class="sd">        r_hat_threshold: If not `None`, specifies a maximum value for the</span>
<span class="sd">            (rank-normalized, split) R-hat convergence diagnostic computed from the</span>
<span class="sd">            chains, with R-hat values exceeding this threshold leading to an</span>
<span class="sd">            exception being raised. Requires `n_chain &gt; 1` and for ArviZ package to</span>
<span class="sd">            be installed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary of parameters corresponding to approximate posterior sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">r_hat_threshold</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ARVIZ_IMPORTED</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;R-hat convergence checks require ArviZ to be installed&quot;</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">value_and_grad_neg_log_marginal_posterior</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">gaussian_process</span><span class="o">.</span><span class="n">neg_log_marginal_posterior</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">grad_neg_log_marginal_posterior</span><span class="p">(</span>
        <span class="n">unconstrained_variables</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">value_and_grad_neg_log_marginal_posterior</span><span class="p">(</span>
            <span class="n">unconstrained_variables</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="n">init_states</span> <span class="o">=</span> <span class="n">gaussian_process</span><span class="o">.</span><span class="n">sample_unconstrained_parameters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_chain</span><span class="p">)</span>

    <span class="n">system</span> <span class="o">=</span> <span class="n">mici</span><span class="o">.</span><span class="n">systems</span><span class="o">.</span><span class="n">EuclideanMetricSystem</span><span class="p">(</span>
        <span class="n">neg_log_dens</span><span class="o">=</span><span class="n">gaussian_process</span><span class="o">.</span><span class="n">neg_log_marginal_posterior</span><span class="p">,</span>
        <span class="n">grad_neg_log_dens</span><span class="o">=</span><span class="n">grad_neg_log_marginal_posterior</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">integrator</span> <span class="o">=</span> <span class="n">mici</span><span class="o">.</span><span class="n">integrators</span><span class="o">.</span><span class="n">LeapfrogIntegrator</span><span class="p">(</span><span class="n">system</span><span class="p">)</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">mici</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">DynamicMultinomialHMC</span><span class="p">(</span><span class="n">system</span><span class="p">,</span> <span class="n">integrator</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

    <span class="n">final_states</span><span class="p">,</span> <span class="n">traces</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">sample_chains</span><span class="p">(</span>
        <span class="n">n_warm_up_iter</span><span class="p">,</span>
        <span class="n">n_main_iter</span><span class="p">,</span>
        <span class="n">init_states</span><span class="p">,</span>
        <span class="n">monitor_stats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accept_stat&quot;</span><span class="p">,</span> <span class="s2">&quot;step_size&quot;</span><span class="p">,</span> <span class="s2">&quot;n_step&quot;</span><span class="p">,</span> <span class="s2">&quot;diverging&quot;</span><span class="p">],</span>
        <span class="n">adapters</span><span class="o">=</span><span class="p">[</span>
            <span class="n">mici</span><span class="o">.</span><span class="n">adapters</span><span class="o">.</span><span class="n">DualAveragingStepSizeAdapter</span><span class="p">(</span><span class="mf">0.8</span><span class="p">),</span>
            <span class="n">mici</span><span class="o">.</span><span class="n">adapters</span><span class="o">.</span><span class="n">OnlineCovarianceMetricAdapter</span><span class="p">(),</span>
        <span class="p">],</span>
        <span class="n">n_process</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">n_chain</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">r_hat_threshold</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_rhat</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">arviz</span><span class="o">.</span><span class="n">rhat</span><span class="p">(</span><span class="n">traces</span><span class="p">)</span><span class="o">.</span><span class="n">to_array</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">max_rhat</span> <span class="o">&gt;</span> <span class="n">r_hat_threshold</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Chain convergence issue: max rank-normalized R-hat </span><span class="si">{</span><span class="n">max_rhat</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gaussian_process</span><span class="o">.</span><span class="n">transform_parameters</span><span class="p">(</span><span class="n">final_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">pos</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.emulation.fit_gaussian_process_parameters_map" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">fit_gaussian_process_parameters_map</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_gaussian_process_parameters_map</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="n">gaussian_process</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">minimize_function</span><span class="o">=</span><span class="n">minimize_with_restarts</span><span class="p">,</span>
    <span class="o">**</span><span class="n">minimize_function_kwargs</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Fit parameters of Gaussian process model by maximimizing posterior density.</p>
<p>Finds maximum-a-posterior (MAP) estimate of Gaussian process parameters by
minimizing negative logarithm of posterior density on parameters given data.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Seeded NumPy random number generator.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>gaussian_process</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.emulation.GaussianProcessModel" href="#calibr.emulation.GaussianProcessModel">GaussianProcessModel</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of functions for Gaussian process model to fit.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_function</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.optimization.GlobalMinimizer" href="#calibr.optimization.GlobalMinimizer">GlobalMinimizer</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function used to attempt to find global minimum of negative
log posterior density function.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="calibr.optimization.minimize_with_restarts" href="#calibr.optimization.minimize_with_restarts">minimize_with_restarts</a></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>**minimize_function_kwargs</code></td>
          <td>
                <code><span title="calibr.optimization.GlobalMinimizerKwarg">GlobalMinimizerKwarg</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Any keyword arguments to pass to
<code>minimize_function</code> function used to optimize negative posterior log density
function.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="emul.types.ParametersDict">ParametersDict</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dictionary of parameters corresponding to maximum-a-posteriori estimate.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/emulation.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_gaussian_process_parameters_map</span><span class="p">(</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="n">gaussian_process</span><span class="p">:</span> <span class="n">GaussianProcessModel</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">minimize_function</span><span class="p">:</span> <span class="n">GlobalMinimizer</span> <span class="o">=</span> <span class="n">minimize_with_restarts</span><span class="p">,</span>
    <span class="o">**</span><span class="n">minimize_function_kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParametersDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit parameters of Gaussian process model by maximimizing posterior density.</span>

<span class="sd">    Finds maximum-a-posterior (MAP) estimate of Gaussian process parameters by</span>
<span class="sd">    minimizing negative logarithm of posterior density on parameters given data.</span>

<span class="sd">    Args:</span>
<span class="sd">        rng: Seeded NumPy random number generator.</span>
<span class="sd">        gaussian_process: Tuple of functions for Gaussian process model to fit.</span>
<span class="sd">        minimize_function: Function used to attempt to find global minimum of negative</span>
<span class="sd">            log posterior density function.</span>
<span class="sd">        **minimize_function_kwargs: Any keyword arguments to pass to</span>
<span class="sd">            `minimize_function` function used to optimize negative posterior log density</span>
<span class="sd">            function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary of parameters corresponding to maximum-a-posteriori estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">minimize_function</span> <span class="ow">is</span> <span class="n">minimize_with_restarts</span><span class="p">:</span>
        <span class="n">minimize_function_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;number_minima_to_find&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">minimize_function_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;maximum_minimize_calls&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">minimize_function_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;minimize_method&quot;</span><span class="p">,</span> <span class="s2">&quot;Newton-CG&quot;</span><span class="p">)</span>
    <span class="n">unconstrained_parameters</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">minimize_function</span><span class="p">(</span>
        <span class="n">objective_function</span><span class="o">=</span><span class="n">gaussian_process</span><span class="o">.</span><span class="n">neg_log_marginal_posterior</span><span class="p">,</span>
        <span class="n">sample_initial_state</span><span class="o">=</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">gaussian_process</span><span class="o">.</span><span class="n">sample_unconstrained_parameters</span><span class="p">(</span>
            <span class="n">r</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
        <span class="o">**</span><span class="n">minimize_function_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">gaussian_process</span><span class="o">.</span><span class="n">transform_parameters</span><span class="p">(</span><span class="n">unconstrained_parameters</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.emulation.get_gaussian_process_factory" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">get_gaussian_process_factory</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">get_gaussian_process_factory</span><span class="p">(</span>
    <span class="n">mean_function</span><span class="p">,</span>
    <span class="n">covariance_function</span><span class="p">,</span>
    <span class="n">neg_log_prior_density</span><span class="p">,</span>
    <span class="n">transform_parameters</span><span class="p">,</span>
    <span class="n">sample_unconstrained_parameters</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Construct a factory function generating Gaussian process models given data.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>mean_function</code></td>
          <td>
                <code><span title="emul.types.MeanFunction">MeanFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Mean function for Gaussian process.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>covariance_function</code></td>
          <td>
                <code><span title="emul.types.CovarianceFunction">CovarianceFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Covariance function for Gaussian process.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>neg_log_prior_density</code></td>
          <td>
                <code><span title="calibr.emulation.NegativeLogPriorDensity">NegativeLogPriorDensity</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Negative logarithm of density of prior distribution on
vector of unconstrained parameters for Gaussian process model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>transform_parameters</code></td>
          <td>
                <code><span title="calibr.emulation.ParameterTransformer">ParameterTransformer</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function which maps flat unconstrained parameter vector to
a dictionary of (potential constrained) parameters, keyed by parameter name.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_unconstrained_parameters</code></td>
          <td>
                <code><span title="calibr.emulation.UnconstrainedParametersSampler">UnconstrainedParametersSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function generating random values for
unconstrained vector of Gaussian process parameters.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="calibr.emulation.GaussianProcessFactory">GaussianProcessFactory</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Gaussian process factory function.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/emulation.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_gaussian_process_factory</span><span class="p">(</span>
    <span class="n">mean_function</span><span class="p">:</span> <span class="n">MeanFunction</span><span class="p">,</span>
    <span class="n">covariance_function</span><span class="p">:</span> <span class="n">CovarianceFunction</span><span class="p">,</span>
    <span class="n">neg_log_prior_density</span><span class="p">:</span> <span class="n">NegativeLogPriorDensity</span><span class="p">,</span>
    <span class="n">transform_parameters</span><span class="p">:</span> <span class="n">ParameterTransformer</span><span class="p">,</span>
    <span class="n">sample_unconstrained_parameters</span><span class="p">:</span> <span class="n">UnconstrainedParametersSampler</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GaussianProcessFactory</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a factory function generating Gaussian process models given data.</span>

<span class="sd">    Args:</span>
<span class="sd">        mean_function: Mean function for Gaussian process.</span>
<span class="sd">        covariance_function: Covariance function for Gaussian process.</span>
<span class="sd">        neg_log_prior_density: Negative logarithm of density of prior distribution on</span>
<span class="sd">            vector of unconstrained parameters for Gaussian process model.</span>
<span class="sd">        transform_parameters: Function which maps flat unconstrained parameter vector to</span>
<span class="sd">            a dictionary of (potential constrained) parameters, keyed by parameter name.</span>
<span class="sd">        sample_unconstrained_parameters: Function generating random values for</span>
<span class="sd">            unconstrained vector of Gaussian process parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Gaussian process factory function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">gaussian_process_factory</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">DataDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GaussianProcessModel</span><span class="p">:</span>
        <span class="p">(</span>
            <span class="n">neg_log_marginal_likelihood</span><span class="p">,</span>
            <span class="n">get_posterior_functions</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">gaussian_process_with_isotropic_gaussian_observations</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">mean_function</span><span class="p">,</span> <span class="n">covariance_function</span>
        <span class="p">)</span>

        <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
        <span class="k">def</span> <span class="nf">neg_log_marginal_posterior</span><span class="p">(</span><span class="n">unconstrained_parameters</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
            <span class="n">parameters</span> <span class="o">=</span> <span class="n">transform_parameters</span><span class="p">(</span><span class="n">unconstrained_parameters</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">neg_log_prior_density</span><span class="p">(</span>
                <span class="n">unconstrained_parameters</span>
            <span class="p">)</span> <span class="o">+</span> <span class="n">neg_log_marginal_likelihood</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">GaussianProcessModel</span><span class="p">(</span>
            <span class="n">neg_log_marginal_posterior</span><span class="p">,</span>
            <span class="n">get_posterior_functions</span><span class="p">,</span>
            <span class="n">transform_parameters</span><span class="p">,</span>
            <span class="n">sample_unconstrained_parameters</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">gaussian_process_factory</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="calibr.optimization" class="doc doc-heading">
          <span class="doc doc-object-name doc-module-name">optimization</span>


</h2>

  <div class="doc doc-contents ">
  
      <p>Functions for minimization of objective functions.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="calibr.optimization.ConvergenceError" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">ConvergenceError</span>


</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/exceptions.html#Exception">Exception</a></code></p>

  
      <p>Error raised when optimizer fails to converge within given computation budget.</p>

            <details class="quote">
              <summary>Source code in <code>src/calibr/optimization.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ConvergenceError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Error raised when optimizer fails to converge within given computation budget.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="calibr.optimization.GlobalMinimizer" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">GlobalMinimizer</span>


</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-external" title="typing.Protocol" href="https://docs.python.org/3/library/typing.html#typing.Protocol">Protocol</a></code></p>

  
      <p>Function which attempts to find global minimum of a scalar objective function.</p>

            <details class="quote">
              <summary>Source code in <code>src/calibr/optimization.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">GlobalMinimizer</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function which attempts to find global minimum of a scalar objective function.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">objective_function</span><span class="p">:</span> <span class="n">ObjectiveFunction</span><span class="p">,</span>
        <span class="n">sample_initial_state</span><span class="p">:</span> <span class="n">InitialStateSampler</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Minimize a differentiable objective function.</span>

<span class="sd">        Args:</span>
<span class="sd">            objective_function: Differentiable scalar-valued function of a single flat</span>
<span class="sd">                vector argument to be minimized. Assumed to be specified using JAX</span>
<span class="sd">                primitives such that its gradient and Hessian can be computed using</span>
<span class="sd">                JAX&#39;s automatic differentiation support, and to be suitable for</span>
<span class="sd">                just-in-time compilation.</span>
<span class="sd">            sample_initial_state: Callable with one argument, which when passed a NumPy</span>
<span class="sd">                random number generator returns a random initial state for optimization</span>
<span class="sd">                of appropriate dimension.</span>
<span class="sd">            rng: Seeded NumPy random number generator.</span>
<span class="sd">            **kwargs: Any keyword arguments to global minimizer function.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple with first entry the state corresponding to the minima point and the</span>
<span class="sd">            second entry the corresponding objective function value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="calibr.optimization.GlobalMinimizer.__call__" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">__call__</span>


</h4>
<div class="doc-signature highlight"><pre><span></span><code><span class="fm">__call__</span><span class="p">(</span><span class="nf">objective_function</span><span class="p">,</span> <span class="n">sample_initial_state</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Minimize a differentiable objective function.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>objective_function</code></td>
          <td>
                <code><span title="calibr.optimization.ObjectiveFunction">ObjectiveFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Differentiable scalar-valued function of a single flat
vector argument to be minimized. Assumed to be specified using JAX
primitives such that its gradient and Hessian can be computed using
JAX's automatic differentiation support, and to be suitable for
just-in-time compilation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_initial_state</code></td>
          <td>
                <code><span title="calibr.optimization.InitialStateSampler">InitialStateSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Callable with one argument, which when passed a NumPy
random number generator returns a random initial state for optimization
of appropriate dimension.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Seeded NumPy random number generator.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>**kwargs</code></td>
          <td>
                <code><span title="calibr.optimization.GlobalMinimizerKwarg">GlobalMinimizerKwarg</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Any keyword arguments to global minimizer function.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple with first entry the state corresponding to the minima point and the</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>second entry the corresponding objective function value.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/optimization.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">objective_function</span><span class="p">:</span> <span class="n">ObjectiveFunction</span><span class="p">,</span>
    <span class="n">sample_initial_state</span><span class="p">:</span> <span class="n">InitialStateSampler</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Minimize a differentiable objective function.</span>

<span class="sd">    Args:</span>
<span class="sd">        objective_function: Differentiable scalar-valued function of a single flat</span>
<span class="sd">            vector argument to be minimized. Assumed to be specified using JAX</span>
<span class="sd">            primitives such that its gradient and Hessian can be computed using</span>
<span class="sd">            JAX&#39;s automatic differentiation support, and to be suitable for</span>
<span class="sd">            just-in-time compilation.</span>
<span class="sd">        sample_initial_state: Callable with one argument, which when passed a NumPy</span>
<span class="sd">            random number generator returns a random initial state for optimization</span>
<span class="sd">            of appropriate dimension.</span>
<span class="sd">        rng: Seeded NumPy random number generator.</span>
<span class="sd">        **kwargs: Any keyword arguments to global minimizer function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple with first entry the state corresponding to the minima point and the</span>
<span class="sd">        second entry the corresponding objective function value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>



<div class="doc doc-object doc-function">



<h3 id="calibr.optimization.basin_hopping" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">basin_hopping</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">basin_hopping</span><span class="p">(</span>
    <span class="n">objective_function</span><span class="p">,</span>
    <span class="n">sample_initial_state</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">minimize_method</span><span class="o">=</span><span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span>
    <span class="n">minimize_max_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">minimize_tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">unknown_kwargs</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Minimize a differentiable objective function with SciPy basin-hopping algorithm.</p>
<p>The basin-hopping algorithm nests an inner local minimization using the
<code>scipy.optimize.minimize</code> method within an outer global stepping algorithm which
perturbs the current state with a random displacement and accepts or rejects this
proposal using a Metropolis criterion.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>objective_function</code></td>
          <td>
                <code><span title="calibr.optimization.ObjectiveFunction">ObjectiveFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Differentiable scalar-valued function of a single flat
vector argument to be minimized. Assumed to be specified using JAX
primitives such that its gradient and Hessian can be computed using JAX's
automatic differentiation support, and to be suitable for just-in-time
compilation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_initial_state</code></td>
          <td>
                <code><span title="calibr.optimization.InitialStateSampler">InitialStateSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Callable with one argument, which when passed a NumPy
random number generator returns a random initial state for optimization of
appropriate dimension.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Seeded NumPy random number generator.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>num_iterations</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of basin-hopping iterations, with number of inner
<code>scipy.optimize.minimize</code> calls being <code>num_iterations + 1</code>.</p>
            </div>
          </td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_method</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>String specifying one of local optimization methods which can
be passed to <code>method</code> argument of <code>scipy.optimize.minimize</code>.</p>
            </div>
          </td>
          <td>
                <code>&#39;Newton-CG&#39;</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_max_iterations</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum number of iterations in inner local
minimization.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_tol</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tolerance parameter for inner local minimization.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple with first entry the state corresponding to the best minima candidate</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>found and the second entry the corresponding objective function value.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/optimization.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">basin_hopping</span><span class="p">(</span>
    <span class="n">objective_function</span><span class="p">:</span> <span class="n">ObjectiveFunction</span><span class="p">,</span>
    <span class="n">sample_initial_state</span><span class="p">:</span> <span class="n">InitialStateSampler</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">minimize_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span>
    <span class="n">minimize_max_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">minimize_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">unknown_kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Minimize a differentiable objective function with SciPy basin-hopping algorithm.</span>

<span class="sd">    The basin-hopping algorithm nests an inner local minimization using the</span>
<span class="sd">    `scipy.optimize.minimize` method within an outer global stepping algorithm which</span>
<span class="sd">    perturbs the current state with a random displacement and accepts or rejects this</span>
<span class="sd">    proposal using a Metropolis criterion.</span>

<span class="sd">    Args:</span>
<span class="sd">        objective_function: Differentiable scalar-valued function of a single flat</span>
<span class="sd">            vector argument to be minimized. Assumed to be specified using JAX</span>
<span class="sd">            primitives such that its gradient and Hessian can be computed using JAX&#39;s</span>
<span class="sd">            automatic differentiation support, and to be suitable for just-in-time</span>
<span class="sd">            compilation.</span>
<span class="sd">        sample_initial_state: Callable with one argument, which when passed a NumPy</span>
<span class="sd">            random number generator returns a random initial state for optimization of</span>
<span class="sd">            appropriate dimension.</span>
<span class="sd">        rng: Seeded NumPy random number generator.</span>
<span class="sd">        num_iterations: Number of basin-hopping iterations, with number of inner</span>
<span class="sd">            `scipy.optimize.minimize` calls being `num_iterations + 1`.</span>
<span class="sd">        minimize_method: String specifying one of local optimization methods which can</span>
<span class="sd">            be passed to `method` argument of `scipy.optimize.minimize`.</span>
<span class="sd">        minimize_max_iterations: Maximum number of iterations in inner local</span>
<span class="sd">            minimization.</span>
<span class="sd">        minimize_tol: Tolerance parameter for inner local minimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple with first entry the state corresponding to the best minima candidate</span>
<span class="sd">        found and the second entry the corresponding objective function value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_unknown_kwargs</span><span class="p">(</span><span class="n">unknown_kwargs</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">_basin_hopping</span><span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">objective_function</span><span class="p">),</span>
        <span class="n">x0</span><span class="o">=</span><span class="n">sample_initial_state</span><span class="p">(</span><span class="n">rng</span><span class="p">),</span>
        <span class="n">niter</span><span class="o">=</span><span class="n">num_iterations</span><span class="p">,</span>
        <span class="n">minimizer_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="n">minimize_method</span><span class="p">,</span>
            <span class="s2">&quot;jac&quot;</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)),</span>
            <span class="s2">&quot;hessp&quot;</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">hessian_vector_product</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)),</span>
            <span class="s2">&quot;tol&quot;</span><span class="p">:</span> <span class="n">minimize_tol</span><span class="p">,</span>
            <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="n">minimize_max_iterations</span><span class="p">},</span>
        <span class="p">},</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">fun</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.optimization.hessian_vector_product" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">hessian_vector_product</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">hessian_vector_product</span><span class="p">(</span><span class="n">scalar_function</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Construct function to compute Hessian-vector product for scalar-valued function.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>scalar_function</code></td>
          <td>
                <code><span title="calibr.optimization.ObjectiveFunction">ObjectiveFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Scalar-valued objective function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a>, <a class="autorefs autorefs-external" title="jax.typing.ArrayLike" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.typing.ArrayLike.html#jax.typing.ArrayLike">ArrayLike</a>], <a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Hessian-vector product function.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/optimization.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">hessian_vector_product</span><span class="p">(</span>
    <span class="n">scalar_function</span><span class="p">:</span> <span class="n">ObjectiveFunction</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">ArrayLike</span><span class="p">,</span> <span class="n">ArrayLike</span><span class="p">],</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct function to compute Hessian-vector product for scalar-valued function.</span>

<span class="sd">    Args:</span>
<span class="sd">        scalar_function: Scalar-valued objective function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Hessian-vector product function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">scalar_function</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="n">v</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">hvp</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="calibr.optimization.minimize_with_restarts" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">minimize_with_restarts</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">minimize_with_restarts</span><span class="p">(</span>
    <span class="n">objective_function</span><span class="p">,</span>
    <span class="n">sample_initial_state</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">number_minima_to_find</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">maximum_minimize_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">minimize_method</span><span class="o">=</span><span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span>
    <span class="n">minimize_max_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">minimize_tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">logging_function</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">unknown_kwargs</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Minimize a differentiable objective function with random restarts.</p>
<p>Iteratively calls <code>scipy.optimize.minimize</code> to attempt to find a minimum of an
objective function until a specified number of candidate minima are successfully
found, with the initial state for each <code>minimize</code> called being randomly sampled
using a user provided function. The candidate minima with the minimum value for
the objective function is returned along with the corresponding objective function
value.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>objective_function</code></td>
          <td>
                <code><span title="calibr.optimization.ObjectiveFunction">ObjectiveFunction</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Differentiable scalar-valued function of a single flat
vector argument to be minimized. Assumed to be specified using JAX
primitives such that its gradient and Hessian can be computed using JAX's
automatic differentiation support, and to be suitable for just-in-time
compilation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>sample_initial_state</code></td>
          <td>
                <code><span title="calibr.optimization.InitialStateSampler">InitialStateSampler</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Callable with one argument, which when passed a NumPy
random number generator returns a random initial state for optimization of
appropriate dimension.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>rng</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="numpy.random.Generator" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator">Generator</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Seeded NumPy random number generator.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Other Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>number_minima_to_find</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of candidate minima of objective function to try
to find.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>maximum_minimize_calls</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum number of times to try calling
<code>scipy.optimize.minimize</code> to find candidate minima. If insufficient
candidates are found within this number of calls then a <code>ConvergenceError</code>
exception is raised.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_method</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>String specifying one of local optimization methods which can
be passed to <code>method</code> argument of <code>scipy.optimize.minimize</code>.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_max_iterations</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum number of iterations in inner local
minimization.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>minimize_tol</code></td>
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tolerance parameter for inner local minimization.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>logging_function</code></td>
          <td>
                <code><a class="autorefs autorefs-external" title="collections.abc.Callable" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable">Callable</a>[[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>], None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Function to use to optionally log status messages during
minimization. Defaults to a no-op function which discards messages.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" title="jax.Array" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array">Array</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple with first entry the state corresponding to the best minima candidate</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>found and the second entry the corresponding objective function value.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/calibr/optimization.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">minimize_with_restarts</span><span class="p">(</span>
    <span class="n">objective_function</span><span class="p">:</span> <span class="n">ObjectiveFunction</span><span class="p">,</span>
    <span class="n">sample_initial_state</span><span class="p">:</span> <span class="n">InitialStateSampler</span><span class="p">,</span>
    <span class="n">rng</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">number_minima_to_find</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">maximum_minimize_calls</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">minimize_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span>
    <span class="n">minimize_max_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">minimize_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logging_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">unknown_kwargs</span><span class="p">:</span> <span class="n">GlobalMinimizerKwarg</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Minimize a differentiable objective function with random restarts.</span>

<span class="sd">    Iteratively calls `scipy.optimize.minimize` to attempt to find a minimum of an</span>
<span class="sd">    objective function until a specified number of candidate minima are successfully</span>
<span class="sd">    found, with the initial state for each `minimize` called being randomly sampled</span>
<span class="sd">    using a user provided function. The candidate minima with the minimum value for</span>
<span class="sd">    the objective function is returned along with the corresponding objective function</span>
<span class="sd">    value.</span>

<span class="sd">    Args:</span>
<span class="sd">        objective_function: Differentiable scalar-valued function of a single flat</span>
<span class="sd">            vector argument to be minimized. Assumed to be specified using JAX</span>
<span class="sd">            primitives such that its gradient and Hessian can be computed using JAX&#39;s</span>
<span class="sd">            automatic differentiation support, and to be suitable for just-in-time</span>
<span class="sd">            compilation.</span>
<span class="sd">        sample_initial_state: Callable with one argument, which when passed a NumPy</span>
<span class="sd">            random number generator returns a random initial state for optimization of</span>
<span class="sd">            appropriate dimension.</span>
<span class="sd">        rng: Seeded NumPy random number generator.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        number_minima_to_find: Number of candidate minima of objective function to try</span>
<span class="sd">            to find.</span>
<span class="sd">        maximum_minimize_calls: Maximum number of times to try calling</span>
<span class="sd">            `scipy.optimize.minimize` to find candidate minima. If insufficient</span>
<span class="sd">            candidates are found within this number of calls then a `ConvergenceError`</span>
<span class="sd">            exception is raised.</span>
<span class="sd">        minimize_method: String specifying one of local optimization methods which can</span>
<span class="sd">            be passed to `method` argument of `scipy.optimize.minimize`.</span>
<span class="sd">        minimize_max_iterations: Maximum number of iterations in inner local</span>
<span class="sd">            minimization.</span>
<span class="sd">        minimize_tol: Tolerance parameter for inner local minimization.</span>
<span class="sd">        logging_function: Function to use to optionally log status messages during</span>
<span class="sd">            minimization. Defaults to a no-op function which discards messages.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple with first entry the state corresponding to the best minima candidate</span>
<span class="sd">        found and the second entry the corresponding objective function value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_unknown_kwargs</span><span class="p">(</span><span class="n">unknown_kwargs</span><span class="p">)</span>
    <span class="n">minima_found</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">minimize_calls</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">minima_found</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">number_minima_to_find</span>
        <span class="ow">and</span> <span class="n">minimize_calls</span> <span class="o">&lt;</span> <span class="n">maximum_minimize_calls</span>
    <span class="p">):</span>
        <span class="n">logging_function</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting minimize call </span><span class="si">{</span><span class="n">minimize_calls</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">_minimize</span><span class="p">(</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">objective_function</span><span class="p">),</span>
            <span class="n">x0</span><span class="o">=</span><span class="n">sample_initial_state</span><span class="p">(</span><span class="n">rng</span><span class="p">),</span>
            <span class="n">jac</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)),</span>
            <span class="n">hessp</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">hessian_vector_product</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)),</span>
            <span class="n">method</span><span class="o">=</span><span class="n">minimize_method</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">minimize_tol</span><span class="p">,</span>
            <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="n">minimize_max_iterations</span><span class="p">},</span>
        <span class="p">)</span>
        <span class="n">minimize_calls</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">results</span><span class="o">.</span><span class="n">success</span><span class="p">:</span>
            <span class="n">logging_function</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found minima with value </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">fun</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># Add minima to minima_found maintaining heap invariant such that first</span>
            <span class="c1"># entry in minima_found is always best solution so far (with counter used</span>
            <span class="c1"># to break ties between solutions with equal values for objective)</span>
            <span class="n">heappush</span><span class="p">(</span><span class="n">minima_found</span><span class="p">,</span> <span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">minima_found</span><span class="p">),</span> <span class="n">results</span><span class="o">.</span><span class="n">x</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging_function</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimization unsuccessful - </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">message</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">minima_found</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">number_minima_to_find</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Did not find required </span><span class="si">{</span><span class="n">number_minima_to_find</span><span class="si">}</span><span class="s2"> minima in &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">maximum_minimize_calls</span><span class="si">}</span><span class="s2"> minimize calls.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="n">ConvergenceError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="c1"># Heap property means first entry in minima_found will correspond to solution</span>
    <span class="c1"># with minimum acquisition function value</span>
    <span class="p">(</span>
        <span class="n">min_objective_function</span><span class="p">,</span>
        <span class="n">_</span><span class="p">,</span>
        <span class="n">state</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">minima_found</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">logging_function</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best minima found has value </span><span class="si">{</span><span class="n">min_objective_function</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">min_objective_function</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>


  </div>

  </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright © 2024 University College London
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/UCL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.action.edit"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.dd8806f2.min.js"></script>
      
    
  </body>
</html>