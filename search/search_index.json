{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#calibr","title":"calibr","text":"<p>Parallelized Bayesian calibration of simulations using Gaussian process emulation.</p> <p><code>calibr</code> is a Python implementation of the algorithm described in Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations (J\u00e4rvenp\u00e4\u00e4, Gutmann, Vehtari and Marttinen; 2021) (doi:10.1214/20-BA1200, arxiv:1905.01252). It is designed to allow estimation of the posterior distribution on the unknown parameters of expensive to evaluate simulator models given observed data, using a batch sequential design strategy which iterates fitting a Gaussian process emulator to a set of evaluations of the (unnormalized) posterior density for the model and using the emulator to identify a new batch of model parameters at which to evaluate the posterior density which minimize a measure of the expected uncertainty in the emulation of the posterior density.</p> <p>The posterior density can be evaluated at the parameter values in each batch in parallel, providing the opportunity for speeding up calibration runs on multi-core and multi-node high performance computing systems. The acquisition functions used to choose new parameter values to evaluate are implemented using the high-performance numerical computing framework JAX, with the gradient-based optimization of these acquisition functions exploiting JAX's support for automatic differentiation.</p> <p>The package is still in the early stages of development, with only a subset of the algorithmic variants proposed by J\u00e4rvenp\u00e4\u00e4, Gutmann, Vehtari and Marttinen (2021) currently implemented. In particular there is no support yet for models with noisy likelihood evaluations. Expect lots of rough edges!</p> <p>This project is developed in collaboration with the Centre for Advanced Research Computing, University College London.</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p><code>calibr</code> requires Python 3.10\u20133.12.</p>"},{"location":"#installation","title":"Installation","text":"<p>We recommend installing in a project specific virtual environment created using a environment management tool such as Mamba or Conda. To install the latest development version of <code>calibr</code> using <code>pip</code> in the currently active environment run</p> <pre><code>pip install git+https://github.com/UCL/calibr.git\n</code></pre> <p>Alternatively create a local clone of the repository with</p> <pre><code>git clone https://github.com/UCL/calibr.git\n</code></pre> <p>and then install in editable mode by running</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation for the package is available at https://github-pages.ucl.ac.uk/calibr/.</p>"},{"location":"#running-tests","title":"Running tests","text":"<p>Tests can be run across all compatible Python versions in isolated environments using <code>tox</code> by running</p> <pre><code>tox\n</code></pre> <p>To run tests manually in a Python environment with <code>pytest</code> installed run</p> <pre><code>pytest tests\n</code></pre> <p>again from the root of the repository.</p>"},{"location":"#building-documentation","title":"Building documentation","text":"<p>The MkDocs HTML documentation can be built locally by running</p> <pre><code>tox -e docs\n</code></pre> <p>from the root of the repository. The built documentation will be written to <code>site</code>.</p> <p>Alternatively to build and preview the documentation locally, in a Python environment with the optional <code>docs</code> dependencies installed, run</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was funded by a grant from the ExCALIBUR programme.</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#the-mit-license-mit","title":"The MIT License (MIT)","text":"<p>Copyright (c) 2023 University College London</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API reference","text":"<p>Bayesian calibration of simulations using Gaussian process emulation.</p>"},{"location":"api/#calibr.acquisition_functions","title":"acquisition_functions","text":"<p>Acquisition functions for selecting new inputs points to evaluate model at.</p>"},{"location":"api/#calibr.acquisition_functions.get_expected_integrated_variance_acquisition_function","title":"get_expected_integrated_variance_acquisition_function","text":"<pre><code>get_expected_integrated_variance_acquisition_function(\n    gp_mean_and_variance,\n    gp_lookahead_variance_reduction,\n    integration_inputs,\n    integration_log_weights,\n)\n</code></pre> <p>Construct acquisition function for minimising expected integrated variance.</p> <p>Selects next input points to evaluate log-likelihood at which minimizes the expectation of the integral over the input space of the variance of an unnormalized posterior density approximation based on a Gaussian process emulator for the log-likelihood function, with the expectation being over the posterior predictive distribution on the unnormalized target density under the Gaussian process.</p> <p>Parameters:</p> Name Type Description Default <code>gp_mean_and_variance</code> <code>PosteriorPredictiveMeanAndVariance</code> <p>Function evaluating mean and variance of Gaussian process emulator for target log-density on input space.</p> required <code>gp_lookahead_variance_reduction</code> <code>PosteriorPredictiveLookaheadVarianceReduction</code> <p>Function evaluating reduction in variance of Gaussian process emulator for log-density at a test-point given one or 'pending' input points, when outputs associated with pending inputs are assumed to follow the posterior predictive distribution under the Gaussian process.</p> required <code>integration_inputs</code> <code>ArrayLike</code> <p>Points to use when approximating integrals over input space.</p> required <code>integration_log_weights</code> <code>ArrayLike</code> <p>Logarithm of weights associated with each of points in <code>integration_inputs</code>.</p> required <p>Returns:</p> Type Description <code>AcquisitionFunction</code> <p>The acquisition function to minimize to select new input point(s).</p> Source code in <code>src/calibr/acquisition_functions.py</code> <pre><code>def get_expected_integrated_variance_acquisition_function(\n    gp_mean_and_variance: PosteriorPredictiveMeanAndVariance,\n    gp_lookahead_variance_reduction: PosteriorPredictiveLookaheadVarianceReduction,\n    integration_inputs: ArrayLike,\n    integration_log_weights: ArrayLike,\n) -&gt; AcquisitionFunction:\n    \"\"\"\n    Construct acquisition function for minimising expected integrated variance.\n\n    Selects next input points to evaluate log-likelihood at which minimizes the\n    expectation of the integral over the input space of the variance of an unnormalized\n    posterior density approximation based on a Gaussian process emulator for the\n    log-likelihood function, with the expectation being over the posterior predictive\n    distribution on the unnormalized target density under the Gaussian process.\n\n    Args:\n        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process\n            emulator for target log-density on input space.\n        gp_lookahead_variance_reduction: Function evaluating reduction in variance of\n            Gaussian process emulator for log-density at a test-point given one or\n            'pending' input points, when outputs associated with pending inputs are\n            assumed to follow the posterior predictive distribution under the Gaussian\n            process.\n        integration_inputs: Points to use when approximating integrals over input space.\n        integration_log_weights: Logarithm of weights associated with each of points\n            in `integration_inputs`.\n\n    Returns:\n        The acquisition function to minimize to select new input point(s).\n    \"\"\"\n    mean_integration_inputs, variance_integration_inputs = jax.vmap(\n        gp_mean_and_variance\n    )(integration_inputs)\n\n    def acquisition_function(new_inputs: ArrayLike) -&gt; Array:\n        lookahead_variance_reduction_integration_inputs = jax.vmap(\n            gp_lookahead_variance_reduction, (0, None)\n        )(integration_inputs, new_inputs)\n        # We neglect the initial constant wrt \u03b8* term in\n        # L\u1d5b\u209c(\u03b8*) = \u222b exp(2m\u209c(\u03b8) + s\u00b2\u209c(\u03b8)) (exp(s\u00b2\u209c(\u03b8)) - exp(\u03c4\u00b2\u209c(\u03b8; \u03b8*))) d\u03b8\n        # and use\n        # -log \u222b exp(2m\u209c(\u03b8) + s\u00b2\u209c(\u03b8) + \u03c4\u00b2\u209c(\u03b8; \u03b8*)) d\u03b8\n        # corresponding to the negative logarithm of the negation of the second term\n        # in the expected integrated variance design criterion.\n        # This appears to give a more numerically stable objective function.\n        return -jsp.special.logsumexp(\n            integration_log_weights\n            + 2 * mean_integration_inputs\n            + variance_integration_inputs\n            + lookahead_variance_reduction_integration_inputs\n        )\n\n    return acquisition_function\n</code></pre>"},{"location":"api/#calibr.acquisition_functions.get_integrated_median_interquantile_range_acquisition_function","title":"get_integrated_median_interquantile_range_acquisition_function","text":"<pre><code>get_integrated_median_interquantile_range_acquisition_function(\n    gp_mean_and_variance,\n    gp_lookahead_variance_reduction,\n    integration_inputs,\n    integration_log_weights,\n    quantile_interval=(0.25, 0.75),\n)\n</code></pre> <p>Construct acquisition function for minimising integrated median interquantile range.</p> <p>Selects next input points to evaluate target log-density at which minimizes the integral over the input space of the median interquantile range of an unnormalized target density approximation based on a Gaussian process emulator for the log-density function, with the median being over the posterior predictive distribution on the unnormalized target density under the Gaussian process.</p> <p>Parameters:</p> Name Type Description Default <code>gp_mean_and_variance</code> <code>PosteriorPredictiveMeanAndVariance</code> <p>Function evaluating mean and variance of Gaussian process emulator for target log-density on input space.</p> required <code>gp_lookahead_variance_reduction</code> <code>PosteriorPredictiveLookaheadVarianceReduction</code> <p>Function evaluating reduction in variance of Gaussian process emulator for log-density at a test-point given one or 'pending' input points, when outputs associated with pending inputs are assumed to follow the posterior predictive distribution under the Gaussian process.</p> required <code>integration_inputs</code> <code>ArrayLike</code> <p>Points to use when approximate integrals over input space.</p> required <code>integration_log_weights</code> <code>ArrayLike</code> <p>Logarithm of weights associated with each of points in <code>integration_inputs</code>.</p> required <code>quantile_interval</code> <code>tuple[float, float]</code> <p>Lower and upper quantiles specifying inter-quantile range to optimize.</p> <code>(0.25, 0.75)</code> <p>Returns:</p> Type Description <code>AcquisitionFunction</code> <p>The acquisition function to minimize to select new input point(s).</p> Source code in <code>src/calibr/acquisition_functions.py</code> <pre><code>def get_integrated_median_interquantile_range_acquisition_function(\n    gp_mean_and_variance: PosteriorPredictiveMeanAndVariance,\n    gp_lookahead_variance_reduction: PosteriorPredictiveLookaheadVarianceReduction,\n    integration_inputs: ArrayLike,\n    integration_log_weights: ArrayLike,\n    quantile_interval: tuple[float, float] = (0.25, 0.75),\n) -&gt; AcquisitionFunction:\n    \"\"\"\n    Construct acquisition function for minimising integrated median interquantile range.\n\n    Selects next input points to evaluate target log-density at which minimizes the\n    integral over the input space of the median interquantile range of an unnormalized\n    target density approximation based on a Gaussian process emulator for the\n    log-density function, with the median being over the posterior predictive\n    distribution on the unnormalized target density under the Gaussian process.\n\n    Args:\n        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process\n            emulator for target log-density on input space.\n        gp_lookahead_variance_reduction: Function evaluating reduction in variance of\n            Gaussian process emulator for log-density at a test-point given one or\n            'pending' input points, when outputs associated with pending inputs are\n            assumed to follow the posterior predictive distribution under the Gaussian\n            process.\n        integration_inputs: Points to use when approximate integrals over input space.\n        integration_log_weights: Logarithm of weights associated with each of points\n            in `integration_inputs`.\n        quantile_interval: Lower and upper quantiles specifying inter-quantile range\n            to optimize.\n\n    Returns:\n        The acquisition function to minimize to select new input point(s).\n    \"\"\"\n    lower = jsp.special.ndtri(quantile_interval[0])\n    upper = jsp.special.ndtri(quantile_interval[1])\n    mean_integration_inputs, variance_integration_inputs = jax.vmap(\n        gp_mean_and_variance\n    )(integration_inputs)\n\n    def acquisition_function(new_inputs: ArrayLike) -&gt; Array:\n        lookahead_variance_reduction_integration_inputs = jax.vmap(\n            gp_lookahead_variance_reduction, (0, None)\n        )(integration_inputs, new_inputs)\n        lookahead_standard_deviation_integration_inputs = (\n            abs(\n                variance_integration_inputs\n                - lookahead_variance_reduction_integration_inputs\n            )\n            ** 0.5\n        )\n        return jsp.special.logsumexp(\n            integration_log_weights\n            + mean_integration_inputs\n            + upper * lookahead_standard_deviation_integration_inputs\n            + jnp.log1p(\n                -jnp.exp(\n                    (lower - upper) * lookahead_standard_deviation_integration_inputs\n                )\n            )\n        )\n\n    return acquisition_function\n</code></pre>"},{"location":"api/#calibr.acquisition_functions.get_maximum_interquantile_range_greedy_batch_acquisition_functions","title":"get_maximum_interquantile_range_greedy_batch_acquisition_functions","text":"<pre><code>get_maximum_interquantile_range_greedy_batch_acquisition_functions(\n    gp_mean_and_variance,\n    gp_lookahead_variance_reduction,\n    quantile_interval=(0.25, 0.75),\n)\n</code></pre> <p>Construct acquisition function for greedy maximisation of interquantile range.</p> <p>Selects next input points to evaluate target log-density at which maximise interquantile range of an unnormalized target density approximation based on a Gaussian process emulator for the log-density function. After an initial point is chosen, subsequent points in the batch are selected in a greedy fashion by maximising the interquantile range given the already selected point(s), assuming the log-density values at the already selected points are distributed according the Gaussian process predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>gp_mean_and_variance</code> <code>PosteriorPredictiveMeanAndVariance</code> <p>Function evaluating mean and variance of Gaussian process emulator for target log-density on input space.</p> required <code>gp_lookahead_variance_reduction</code> <code>PosteriorPredictiveLookaheadVarianceReduction</code> <p>Function evaluating reduction in variance of Gaussian process emulator for log-density at a test-point given one or 'pending' input points, when outputs associated with pending inputs are assumed to follow the posterior predictive distribution under the Gaussian process.</p> required <code>quantile_interval</code> <code>tuple[float, float]</code> <p>Lower and upper quantiles specifying inter-quantile range to optimize.</p> <code>(0.25, 0.75)</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The acquisition function to minimize to select new input point(s). The function</p> <code>Callable</code> <p>takes one or two arguments. If a single argument is passed it corresponds to</p> <code>Callable</code> <p>the acquisition function for the initial point in a batch. If two arguments are</p> <code>Callable</code> <p>passed, the first argument corresponds to the new input point being chosen and</p> <code>Callable</code> <p>the second argument to the already selected input point(s) in the batch.</p> Source code in <code>src/calibr/acquisition_functions.py</code> <pre><code>def get_maximum_interquantile_range_greedy_batch_acquisition_functions(\n    gp_mean_and_variance: PosteriorPredictiveMeanAndVariance,\n    gp_lookahead_variance_reduction: PosteriorPredictiveLookaheadVarianceReduction,\n    quantile_interval: tuple[float, float] = (0.25, 0.75),\n) -&gt; Callable:\n    \"\"\"\n    Construct acquisition function for greedy maximisation of interquantile range.\n\n    Selects next input points to evaluate target log-density at which maximise\n    interquantile range of an unnormalized target density approximation based on a\n    Gaussian process emulator for the log-density function. After an initial point is\n    chosen, subsequent points in the batch are selected in a greedy fashion by\n    maximising the interquantile range given the already selected point(s), assuming the\n    log-density values at the already selected points are distributed according the\n    Gaussian process predictive distribution.\n\n    Args:\n        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process\n            emulator for target log-density on input space.\n        gp_lookahead_variance_reduction: Function evaluating reduction in variance of\n            Gaussian process emulator for log-density at a test-point given one or\n            'pending' input points, when outputs associated with pending inputs are\n            assumed to follow the posterior predictive distribution under the Gaussian\n            process.\n        quantile_interval: Lower and upper quantiles specifying inter-quantile range\n            to optimize.\n\n    Returns:\n        The acquisition function to minimize to select new input point(s). The function\n        takes one or two arguments. If a single argument is passed it corresponds to\n        the acquisition function for the initial point in a batch. If two arguments are\n        passed, the first argument corresponds to the new input point being chosen and\n        the second argument to the already selected input point(s) in the batch.\n    \"\"\"\n    lower = jsp.special.ndtri(quantile_interval[0])\n    upper = jsp.special.ndtri(quantile_interval[1])\n\n    def acquisition_function(\n        new_input: ArrayLike, pending_inputs: ArrayLike | None = None\n    ) -&gt; Array:\n        mean, variance = gp_mean_and_variance(new_input)\n        if pending_inputs is None:\n            lookahead_variance_reduction = 0\n        else:\n            lookahead_variance_reduction = gp_lookahead_variance_reduction(\n                new_input, pending_inputs\n            )\n        lookahead_standard_deviation = (\n            abs(variance - lookahead_variance_reduction) ** 0.5\n        )\n        return (\n            -mean\n            - upper * lookahead_standard_deviation\n            - jnp.log1p(-jnp.exp(lookahead_standard_deviation * (lower - upper)))\n        )\n\n    return acquisition_function\n</code></pre>"},{"location":"api/#calibr.acquisition_functions.get_maximum_variance_greedy_batch_acquisition_functions","title":"get_maximum_variance_greedy_batch_acquisition_functions","text":"<pre><code>get_maximum_variance_greedy_batch_acquisition_functions(\n    gp_mean_and_variance, gp_lookahead_variance_reduction\n)\n</code></pre> <p>Construct acquisition functions for greedy maximisation of variance.</p> <p>Selects next input points to evaluate target log-density at which maximise variance of an unnormalized target density approximation based on a Gaussian process emulator for the log-density function. After an initial point is chosen, subsequent points in the batch are selected in a greedy fashion by maximising the variance given the already selected point(s), assuming the log-density values at the already selected points are distributed according the Gaussian process predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>gp_mean_and_variance</code> <code>PosteriorPredictiveMeanAndVariance</code> <p>Function evaluating mean and variance of Gaussian process emulator for target log-density on input space.</p> required <code>gp_lookahead_variance_reduction</code> <code>PosteriorPredictiveLookaheadVarianceReduction</code> <p>Function evaluating reduction in variance of Gaussian process emulator for log-density at a test-point given one or 'pending' input points, when outputs associated with pending inputs are assumed to follow the posterior predictive distribution under the Gaussian process.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The acquisition function to minimize to select new input point(s). The function</p> <code>Callable</code> <p>takes one or two arguments. If a single argument is passed it corresponds to</p> <code>Callable</code> <p>the acquisition function for the initial point in a batch. If two arguments are</p> <code>Callable</code> <p>passed, the first argument corresponds to the new input point being chosen and</p> <code>Callable</code> <p>the second argument to the already selected input point(s) in the batch.</p> Source code in <code>src/calibr/acquisition_functions.py</code> <pre><code>def get_maximum_variance_greedy_batch_acquisition_functions(\n    gp_mean_and_variance: PosteriorPredictiveMeanAndVariance,\n    gp_lookahead_variance_reduction: PosteriorPredictiveLookaheadVarianceReduction,\n) -&gt; Callable:\n    \"\"\"\n    Construct acquisition functions for greedy maximisation of variance.\n\n    Selects next input points to evaluate target log-density at which maximise variance\n    of an unnormalized target density approximation based on a Gaussian process emulator\n    for the log-density function. After an initial point is chosen, subsequent points in\n    the batch are selected in a greedy fashion by maximising the variance given the\n    already selected point(s), assuming the log-density values at the already selected\n    points are distributed according the Gaussian process predictive distribution.\n\n    Args:\n        gp_mean_and_variance: Function evaluating mean and variance of Gaussian process\n            emulator for target log-density on input space.\n        gp_lookahead_variance_reduction: Function evaluating reduction in variance of\n            Gaussian process emulator for log-density at a test-point given one or\n            'pending' input points, when outputs associated with pending inputs are\n            assumed to follow the posterior predictive distribution under the Gaussian\n            process.\n\n    Returns:\n        The acquisition function to minimize to select new input point(s). The function\n        takes one or two arguments. If a single argument is passed it corresponds to\n        the acquisition function for the initial point in a batch. If two arguments are\n        passed, the first argument corresponds to the new input point being chosen and\n        the second argument to the already selected input point(s) in the batch.\n    \"\"\"\n\n    def acquisition_function(\n        new_input: ArrayLike, pending_inputs: ArrayLike | None = None\n    ) -&gt; Array:\n        mean, variance = gp_mean_and_variance(new_input)\n        if pending_inputs is None:\n            lookahead_variance_reduction = 0\n        else:\n            lookahead_variance_reduction = gp_lookahead_variance_reduction(\n                new_input, pending_inputs\n            )\n        lookahead_variance = variance - lookahead_variance_reduction\n        return -2 * (mean + variance) - jnp.log1p(-jnp.exp(-lookahead_variance))\n\n    return acquisition_function\n</code></pre>"},{"location":"api/#calibr.calibration","title":"calibration","text":"<p>Functions for iteratively calibrating the parameters of a probabilistic model.</p>"},{"location":"api/#calibr.calibration.calibrate","title":"calibrate","text":"<pre><code>calibrate(\n    num_initial_inputs,\n    batch_size,\n    num_iterations,\n    rng,\n    sample_initial_inputs,\n    posterior_log_density_batch,\n    gaussian_process_factory,\n    get_integration_points_and_log_weights,\n    *,\n    fit_gaussian_process_parameters=fit_gaussian_process_parameters_map,\n    get_acquisition_function=get_integrated_median_interquantile_range_acquisition_function,\n    get_next_inputs_batch=get_next_inputs_batch_by_joint_optimization,\n    end_of_iteration_callback=None\n)\n</code></pre> <p>Estimate the posterior on the unknown inputs of an (expensive to evaluate) model.</p> <p>Iterates evaluating log density for posterior at a batch of inputs (unknown variables to infer posterior on), fitting a Gaussian process to the log density evaluations so far and optimizing an acquisition function using the Gaussian process emulator to choose a new batch of input points at which to evaluate the log density (by minimizing a measure of the expected uncertainty in the emulator about the log posterior density function).</p> <p>Parameters:</p> Name Type Description Default <code>num_initial_inputs</code> <code>int</code> <p>Number of initial inputs to evaluate posterior log density at to initialize calibration.</p> required <code>batch_size</code> <code>int</code> <p>Size of batch of inputs to optimize for and evaluate log density at in each calibration iteration.</p> required <code>num_iterations</code> <code>int</code> <p>Number of calibration iterations to perform. Total number of model posterior log density evaluations is <code>num_initial_inputs + batch_size * num_iterations</code>.</p> required <code>rng</code> <code>Generator</code> <p>Seeded NumPy random number generator.</p> required <code>sample_initial_inputs</code> <code>InitialInputSampler</code> <p>Function outputting reasonable random initial values for batch of inputs when passed a random number generator and batch size.</p> required <code>posterior_log_density_batch</code> <code>Callable[[ArrayLike], Array]</code> <p>Function computing logarithm of (unnormalized) posterior density on model inputs, for a batch of inputs (with passed argument being a two dimensional array with first dimension the batch index).</p> required <code>gaussian_process_factory</code> <code>GaussianProcessFactory</code> <p>Factory function generating Gaussian process models given a data dictionary.</p> required <code>get_integration_points_and_log_weights</code> <code>Callable[[Generator, InitialInputSampler, PosteriorPredictiveMeanAndVariance], tuple[Array, Array]]</code> <p>Function which outputs points in input space and corresponding (log) weights by which to estimate integrals over the input space in the acquisition function as a weighted sum. The function is passed a seeded random number generator, a function to sample random points in the input space and the current Gaussian process posterior predictive mean and variance function. The input points and weights may for example be generated according to a (deterministic) numerical quadrature rule for low dimensionalities, a stochastic (quasi-) Monte Carlo scheme for moderate dimensionalities or a Markov chain Monte Carlo or sequential Monte Carlo scheme for higher dimensionalities.</p> required <code>fit_gaussian_process_parameters</code> <code>GaussianProcessParameterFitter</code> <p>Function which fits the parameters of a Gaussian process model given the current data (input-output pairs). Passed a seeded random number generator and tuple of Gaussian process model functions.</p> <code>fit_gaussian_process_parameters_map</code> <code>get_acquisition_function</code> <code>AcquisitionFunctionFactory</code> <p>Factory function generating acquisition functions given Gaussian process posterior predictive functions.</p> <code>get_integrated_median_interquantile_range_acquisition_function</code> <code>get_next_inputs_batch</code> <code>Callable[[Generator, AcquisitionFunction, InitialInputSampler, int], tuple[Array, float]]</code> <p>Function which computes next batch of inputs to evaluate model at by optimizing the current acquisition function. Passed a seeded random number generator, acquisition function, input sampler and batch size.</p> <code>get_next_inputs_batch_by_joint_optimization</code> <code>end_of_iteration_callback</code> <code>EndOfIterationCallback | None</code> <p>Optional callback function evaluate at end of each calibration iteration, for example for logging metrics or plotting / saving intermediate outputs. Passed current iteration index, Gaussian process posterior mean and variance, data dictionary with all inputs and corresponding log density evaluations so far, batch of inputs selected in current iteration and corresponding optimized acquisition function value.</p> <code>None</code> <p>Returns:</p> Type Description <code>GaussianProcessModel</code> <p>Tuple of Gaussian process model, data dictionary containing all model inputs</p> <code>DataDict</code> <p>and log density evaluations and fitted Gaussian process model parameters at</p> <code>ParametersDict</code> <p>final iteration.</p> Source code in <code>src/calibr/calibration.py</code> <pre><code>def calibrate(  # noqa: PLR0913\n    num_initial_inputs: int,\n    batch_size: int,\n    num_iterations: int,\n    rng: Generator,\n    sample_initial_inputs: InitialInputSampler,\n    posterior_log_density_batch: Callable[[ArrayLike], Array],\n    gaussian_process_factory: GaussianProcessFactory,\n    get_integration_points_and_log_weights: Callable[\n        [Generator, InitialInputSampler, PosteriorPredictiveMeanAndVariance],\n        tuple[Array, Array],\n    ],\n    *,\n    fit_gaussian_process_parameters: GaussianProcessParameterFitter = (\n        fit_gaussian_process_parameters_map\n    ),\n    get_acquisition_function: AcquisitionFunctionFactory = (\n        get_integrated_median_interquantile_range_acquisition_function\n    ),\n    get_next_inputs_batch: Callable[\n        [Generator, AcquisitionFunction, InitialInputSampler, int], tuple[Array, float]\n    ] = get_next_inputs_batch_by_joint_optimization,\n    end_of_iteration_callback: EndOfIterationCallback | None = None,\n) -&gt; tuple[GaussianProcessModel, DataDict, ParametersDict]:\n    \"\"\"\n    Estimate the posterior on the unknown inputs of an (expensive to evaluate) model.\n\n    Iterates evaluating log density for posterior at a batch of inputs (unknown\n    variables to infer posterior on), fitting a Gaussian process to the log density\n    evaluations so far and optimizing an acquisition function using the Gaussian process\n    emulator to choose a new batch of input points at which to evaluate the log density\n    (by minimizing a measure of the expected uncertainty in the emulator about the log\n    posterior density function).\n\n    Args:\n        num_initial_inputs: Number of initial inputs to evaluate posterior log density\n            at to initialize calibration.\n        batch_size: Size of batch of inputs to optimize for and evaluate log density at\n            in each calibration iteration.\n        num_iterations: Number of calibration iterations to perform. Total number of\n            model posterior log density evaluations is\n            `num_initial_inputs + batch_size * num_iterations`.\n        rng: Seeded NumPy random number generator.\n        sample_initial_inputs: Function outputting reasonable random initial values for\n            batch of inputs when passed a random number generator and batch size.\n        posterior_log_density_batch: Function computing logarithm of (unnormalized)\n            posterior density on model inputs, for a batch of inputs (with passed\n            argument being a two dimensional array with first dimension the batch\n            index).\n        gaussian_process_factory: Factory function generating Gaussian process models\n            given a data dictionary.\n        get_integration_points_and_log_weights: Function which outputs points in input\n            space and corresponding (log) weights by which to estimate integrals over\n            the input space in the acquisition function as a weighted sum. The function\n            is passed a seeded random number generator, a function to sample random\n            points in the input space and the current Gaussian process posterior\n            predictive mean and variance function. The input points and weights may for\n            example be generated according to a (deterministic) numerical quadrature\n            rule for low dimensionalities, a stochastic (quasi-) Monte Carlo scheme for\n            moderate dimensionalities or a Markov chain Monte Carlo or sequential Monte\n            Carlo scheme for higher dimensionalities.\n        fit_gaussian_process_parameters: Function which fits the parameters of a\n            Gaussian process model given the current data (input-output pairs). Passed\n            a seeded random number generator and tuple of Gaussian process model\n            functions.\n        get_acquisition_function: Factory function generating acquisition functions\n            given Gaussian process posterior predictive functions.\n        get_next_inputs_batch: Function which computes next batch of inputs to evaluate\n            model at by optimizing the current acquisition function. Passed a seeded\n            random number generator, acquisition function, input sampler and batch size.\n        end_of_iteration_callback: Optional callback function evaluate at end of each\n            calibration iteration, for example for logging metrics or plotting / saving\n            intermediate outputs. Passed current iteration index, Gaussian process\n            posterior mean and variance, data dictionary with all inputs and\n            corresponding log density evaluations so far, batch of inputs selected in\n            current iteration and corresponding optimized acquisition function value.\n\n    Returns:\n        Tuple of Gaussian process model, data dictionary containing all model inputs\n        and log density evaluations and fitted Gaussian process model parameters at\n        final iteration.\n    \"\"\"\n    inputs = sample_initial_inputs(rng, num_initial_inputs)\n    data = {\"inputs\": inputs, \"outputs\": posterior_log_density_batch(inputs)}\n    for iteration_index in range(num_iterations + 1):\n        gaussian_process = gaussian_process_factory(data)\n        parameters = fit_gaussian_process_parameters(rng, gaussian_process)\n        (\n            posterior_mean_and_variance,\n            lookahead_variance_reduction,\n        ) = gaussian_process.get_posterior_functions(parameters)\n        if iteration_index == num_iterations:\n            # In final iteration, only fit Gaussian process to all model evaluations\n            # computed so far without acquiring a new set of inputs to evaluate\n            break\n        (\n            integration_inputs,\n            integration_log_weights,\n        ) = get_integration_points_and_log_weights(\n            rng, sample_initial_inputs, posterior_mean_and_variance\n        )\n        acquisition_function = get_acquisition_function(\n            posterior_mean_and_variance,\n            lookahead_variance_reduction,\n            integration_inputs,\n            integration_log_weights,\n        )\n        next_inputs, acquisition_function_value = get_next_inputs_batch(\n            rng, acquisition_function, sample_initial_inputs, batch_size\n        )\n        next_outputs = posterior_log_density_batch(next_inputs)\n        data = {\n            \"inputs\": np.concatenate((data[\"inputs\"], next_inputs)),\n            \"outputs\": np.concatenate((data[\"outputs\"], next_outputs)),\n        }\n        if end_of_iteration_callback is not None:\n            end_of_iteration_callback(\n                iteration_index,\n                posterior_mean_and_variance,\n                data,\n                next_inputs,\n                acquisition_function_value,\n            )\n    return gaussian_process, data, parameters\n</code></pre>"},{"location":"api/#calibr.calibration.get_next_inputs_batch_by_greedy_optimization","title":"get_next_inputs_batch_by_greedy_optimization","text":"<pre><code>get_next_inputs_batch_by_greedy_optimization(\n    rng,\n    acquisition_function,\n    sample_initial_inputs,\n    batch_size,\n    *,\n    minimize_function=minimize_with_restarts,\n    **minimize_function_kwargs\n)\n</code></pre> <p>Get next batch of inputs to evaluate by greedily optimizing acquisition function.</p> <p>Sequentially minimizes acquisition function for <code>b</code> in 1 to <code>batch_size</code> by fixing <code>b - 1</code> inputs already optimized and minimizing over a single new input in each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random number generator for initializing optimization runs.</p> required <code>acquisition_function</code> <code>AcquisitionFunction</code> <p>Scalar-valued function of a batch of inputs to optimize to find new batch of inputs to evaluate model for.</p> required <code>sample_initial_inputs</code> <code>InitialInputSampler</code> <p>Function outputting reasonable random initial values for batch of inputs when passed a random number generator and batch size. Used to initialize state for optimization runs.</p> required <code>batch_size</code> <code>int</code> <p>Number of inputs in batch.</p> required <code>minimize_function</code> <code>GlobalMinimizer</code> <p>Function used to attempt to find minimum of (sequence of) acquisition functions.</p> <code>minimize_with_restarts</code> <code>**minimize_function_kwargs</code> <code>GlobalMinimizerKwarg</code> <p>Any keyword arguments to pass to <code>minimize_function</code> function used to optimize acquisition function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Array, float]</code> <p>Tuple of optimized inputs batch and corresponding value of acquisition function.</p> Source code in <code>src/calibr/calibration.py</code> <pre><code>def get_next_inputs_batch_by_greedy_optimization(\n    rng: Generator,\n    acquisition_function: AcquisitionFunction,\n    sample_initial_inputs: InitialInputSampler,\n    batch_size: int,\n    *,\n    minimize_function: GlobalMinimizer = minimize_with_restarts,\n    **minimize_function_kwargs: GlobalMinimizerKwarg,\n) -&gt; tuple[Array, float]:\n    \"\"\"\n    Get next batch of inputs to evaluate by greedily optimizing acquisition function.\n\n    Sequentially minimizes acquisition function for `b` in 1 to `batch_size` by fixing\n    `b - 1` inputs already optimized and minimizing over a single new input in each\n    iteration.\n\n    Args:\n        rng: NumPy random number generator for initializing optimization runs.\n        acquisition_function: Scalar-valued function of a batch of inputs to optimize to\n            find new batch of inputs to evaluate model for.\n        sample_initial_inputs: Function outputting reasonable random initial values for\n            batch of inputs when passed a random number generator and batch size. Used\n            to initialize state for optimization runs.\n        batch_size: Number of inputs in batch.\n        minimize_function: Function used to attempt to find minimum of (sequence of)\n            acquisition functions.\n        **minimize_function_kwargs: Any keyword arguments to pass to\n            `minimize_function` function used to optimize acquisition function.\n\n    Returns:\n        Tuple of optimized inputs batch and corresponding value of acquisition function.\n    \"\"\"\n\n    def acquisition_function_greedy(\n        current_input: ArrayLike, fixed_inputs: list[ArrayLike]\n    ) -&gt; float:\n        return acquisition_function(jnp.stack([current_input, *fixed_inputs]))\n\n    fixed_inputs: list[ArrayLike] = []\n    for _ in range(batch_size):\n        current_input, min_acquisition_function = minimize_function(\n            objective_function=partial(\n                acquisition_function_greedy, fixed_inputs=fixed_inputs\n            ),\n            sample_initial_state=lambda r: sample_initial_inputs(r, 1).flatten(),\n            rng=rng,\n            **minimize_function_kwargs,\n        )\n        fixed_inputs.append(current_input)\n\n    return np.stack(fixed_inputs), min_acquisition_function\n</code></pre>"},{"location":"api/#calibr.calibration.get_next_inputs_batch_by_joint_optimization","title":"get_next_inputs_batch_by_joint_optimization","text":"<pre><code>get_next_inputs_batch_by_joint_optimization(\n    rng,\n    acquisition_function,\n    sample_initial_inputs,\n    batch_size,\n    *,\n    minimize_function=minimize_with_restarts,\n    **minimize_function_kwargs\n)\n</code></pre> <p>Get next batch of inputs to evaluate by jointly optimizing acquisition function.</p> <p>Minimizes acquisition function over product of <code>batch_size</code> input spaces.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random number generator for initializing optimization runs.</p> required <code>acquisition_function</code> <code>AcquisitionFunction</code> <p>Scalar-valued function of a batch of inputs to optimize to find new batch of inputs to evaluate model for.</p> required <code>sample_initial_inputs</code> <code>InitialInputSampler</code> <p>Function outputting reasonable random initial values for batch of inputs when passed a random number generator and batch size. Used to initialize state for optimization runs.</p> required <code>batch_size</code> <code>int</code> <p>Number of inputs in batch.</p> required <code>minimize_function</code> <code>GlobalMinimizer</code> <p>Function used to attempt to find minimum of acquisition function.</p> <code>minimize_with_restarts</code> <code>**minimize_function_kwargs</code> <code>GlobalMinimizerKwarg</code> <p>Any keyword arguments to pass to <code>minimize_function</code> function used to optimize acquisition function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Array, float]</code> <p>Tuple of optimized inputs batch and corresponding value of acquisition function.</p> Source code in <code>src/calibr/calibration.py</code> <pre><code>def get_next_inputs_batch_by_joint_optimization(\n    rng: Generator,\n    acquisition_function: AcquisitionFunction,\n    sample_initial_inputs: InitialInputSampler,\n    batch_size: int,\n    *,\n    minimize_function: GlobalMinimizer = minimize_with_restarts,\n    **minimize_function_kwargs: GlobalMinimizerKwarg,\n) -&gt; tuple[Array, float]:\n    \"\"\"\n    Get next batch of inputs to evaluate by jointly optimizing acquisition function.\n\n    Minimizes acquisition function over product of `batch_size` input spaces.\n\n    Args:\n        rng: NumPy random number generator for initializing optimization runs.\n        acquisition_function: Scalar-valued function of a batch of inputs to optimize to\n            find new batch of inputs to evaluate model for.\n        sample_initial_inputs: Function outputting reasonable random initial values for\n            batch of inputs when passed a random number generator and batch size. Used\n            to initialize state for optimization runs.\n        batch_size: Number of inputs in batch.\n        minimize_function: Function used to attempt to find minimum of acquisition\n            function.\n        **minimize_function_kwargs: Any keyword arguments to pass to\n            `minimize_function` function used to optimize acquisition function.\n\n    Returns:\n        Tuple of optimized inputs batch and corresponding value of acquisition function.\n    \"\"\"\n\n    def acquisition_function_flat_input(flat_inputs: ArrayLike) -&gt; float:\n        return acquisition_function(flat_inputs.reshape((batch_size, -1)))\n\n    if minimize_function is minimize_with_restarts:\n        minimize_function_kwargs.setdefault(\"number_minima_to_find\", 5)\n        minimize_function_kwargs.setdefault(\"maximum_minimize_calls\", 100)\n        minimize_function_kwargs.setdefault(\"minimize_method\", \"Newton-CG\")\n\n    flat_inputs, min_acquisition_function = minimize_function(\n        objective_function=acquisition_function_flat_input,\n        sample_initial_state=lambda r: sample_initial_inputs(r, batch_size).flatten(),\n        rng=rng,\n        **minimize_function_kwargs,\n    )\n\n    return flat_inputs.reshape((batch_size, -1)), min_acquisition_function\n</code></pre>"},{"location":"api/#calibr.emulation","title":"emulation","text":"<p>Functions and types for constructing and fitting Gaussian process emulators.</p>"},{"location":"api/#calibr.emulation.GaussianProcessModel","title":"GaussianProcessModel","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Wrapper for functions associated with a Gaussian process model.</p> Source code in <code>src/calibr/emulation.py</code> <pre><code>class GaussianProcessModel(NamedTuple):\n    \"\"\"Wrapper for functions associated with a Gaussian process model.\"\"\"\n\n    neg_log_marginal_posterior: Callable[[ArrayLike], float]\n    get_posterior_functions: PosteriorPredictiveFunctionFactory\n    transform_parameters: ParameterTransformer\n    sample_unconstrained_parameters: UnconstrainedParametersSampler\n</code></pre>"},{"location":"api/#calibr.emulation.fit_gaussian_process_parameters_hmc","title":"fit_gaussian_process_parameters_hmc","text":"<pre><code>fit_gaussian_process_parameters_hmc(\n    rng,\n    gaussian_process,\n    *,\n    n_chain=1,\n    n_warm_up_iter=500,\n    n_main_iter=1000,\n    r_hat_threshold=None\n)\n</code></pre> <p>Fit parameters of Gaussian process model by sampling posterior using HMC.</p> <p>Uses Hamiltonian Monte Carlo (HMC) to generate chain(s) of samples approximating posterior distribution on Gaussian process parameters given data.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Seeded NumPy random number generator.</p> required <code>gaussian_process</code> <code>GaussianProcessModel</code> <p>Tuple of functions for Gaussian process model to fit.</p> required <code>n_chain</code> <code>int</code> <p>Number of Markov chains to simulate.</p> <code>1</code> <code>n_warm_up_iter</code> <code>int</code> <p>Number of adaptive warm-up iterations to run for each chain.</p> <code>500</code> <code>n_main_iter</code> <code>int</code> <p>Number of main sampling stage iterations to run for each chain.</p> <code>1000</code> <code>r_hat_threshold</code> <code>float | None</code> <p>If not <code>None</code>, specifies a maximum value for the (rank-normalized, split) R-hat convergence diagnostic computed from the chains, with R-hat values exceeding this threshold leading to an exception being raised. Requires <code>n_chain &gt; 1</code> and for ArviZ package to be installed.</p> <code>None</code> <p>Returns:</p> Type Description <code>ParametersDict</code> <p>Dictionary of parameters corresponding to approximate posterior sample.</p> Source code in <code>src/calibr/emulation.py</code> <pre><code>def fit_gaussian_process_parameters_hmc(\n    rng: Generator,\n    gaussian_process: GaussianProcessModel,\n    *,\n    n_chain: int = 1,\n    n_warm_up_iter: int = 500,\n    n_main_iter: int = 1000,\n    r_hat_threshold: float | None = None,\n) -&gt; ParametersDict:\n    \"\"\"\n    Fit parameters of Gaussian process model by sampling posterior using HMC.\n\n    Uses Hamiltonian Monte Carlo (HMC) to generate chain(s) of samples approximating\n    posterior distribution on Gaussian process parameters given data.\n\n    Args:\n        rng: Seeded NumPy random number generator.\n        gaussian_process: Tuple of functions for Gaussian process model to fit.\n        n_chain: Number of Markov chains to simulate.\n        n_warm_up_iter: Number of adaptive warm-up iterations to run for each chain.\n        n_main_iter: Number of main sampling stage iterations to run for each chain.\n        r_hat_threshold: If not `None`, specifies a maximum value for the\n            (rank-normalized, split) R-hat convergence diagnostic computed from the\n            chains, with R-hat values exceeding this threshold leading to an\n            exception being raised. Requires `n_chain &gt; 1` and for ArviZ package to\n            be installed.\n\n    Returns:\n        Dictionary of parameters corresponding to approximate posterior sample.\n    \"\"\"\n    if r_hat_threshold is not None and not ARVIZ_IMPORTED:\n        msg = \"R-hat convergence checks require ArviZ to be installed\"\n        raise RuntimeError(msg)\n    value_and_grad_neg_log_marginal_posterior = jax.jit(\n        jax.value_and_grad(gaussian_process.neg_log_marginal_posterior)\n    )\n\n    def grad_neg_log_marginal_posterior(\n        unconstrained_variables: ArrayLike,\n    ) -&gt; tuple[Array, float]:\n        value, grad = value_and_grad_neg_log_marginal_posterior(\n            unconstrained_variables\n        )\n        return np.asarray(grad), float(value)\n\n    init_states = gaussian_process.sample_unconstrained_parameters(rng, n_chain)\n\n    system = mici.systems.EuclideanMetricSystem(\n        neg_log_dens=gaussian_process.neg_log_marginal_posterior,\n        grad_neg_log_dens=grad_neg_log_marginal_posterior,\n    )\n    integrator = mici.integrators.LeapfrogIntegrator(system)\n    sampler = mici.samplers.DynamicMultinomialHMC(system, integrator, rng)\n\n    final_states, traces, _ = sampler.sample_chains(\n        n_warm_up_iter,\n        n_main_iter,\n        init_states,\n        monitor_stats=[\"accept_stat\", \"step_size\", \"n_step\", \"diverging\"],\n        adapters=[\n            mici.adapters.DualAveragingStepSizeAdapter(0.8),\n            mici.adapters.OnlineCovarianceMetricAdapter(),\n        ],\n        n_process=1,\n    )\n\n    if n_chain &gt; 1 and r_hat_threshold is not None:\n        max_rhat = float(arviz.rhat(traces).to_array().max())\n        if max_rhat &gt; r_hat_threshold:\n            msg = f\"Chain convergence issue: max rank-normalized R-hat {max_rhat}\"\n            raise RuntimeError(msg)\n    return gaussian_process.transform_parameters(final_states[0].pos)\n</code></pre>"},{"location":"api/#calibr.emulation.fit_gaussian_process_parameters_map","title":"fit_gaussian_process_parameters_map","text":"<pre><code>fit_gaussian_process_parameters_map(\n    rng,\n    gaussian_process,\n    *,\n    minimize_function=minimize_with_restarts,\n    **minimize_function_kwargs\n)\n</code></pre> <p>Fit parameters of Gaussian process model by maximimizing posterior density.</p> <p>Finds maximum-a-posterior (MAP) estimate of Gaussian process parameters by minimizing negative logarithm of posterior density on parameters given data.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Seeded NumPy random number generator.</p> required <code>gaussian_process</code> <code>GaussianProcessModel</code> <p>Tuple of functions for Gaussian process model to fit.</p> required <code>minimize_function</code> <code>GlobalMinimizer</code> <p>Function used to attempt to find global minimum of negative log posterior density function.</p> <code>minimize_with_restarts</code> <code>**minimize_function_kwargs</code> <code>GlobalMinimizerKwarg</code> <p>Any keyword arguments to pass to <code>minimize_function</code> function used to optimize negative posterior log density function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ParametersDict</code> <p>Dictionary of parameters corresponding to maximum-a-posteriori estimate.</p> Source code in <code>src/calibr/emulation.py</code> <pre><code>def fit_gaussian_process_parameters_map(\n    rng: Generator,\n    gaussian_process: GaussianProcessModel,\n    *,\n    minimize_function: GlobalMinimizer = minimize_with_restarts,\n    **minimize_function_kwargs: GlobalMinimizerKwarg,\n) -&gt; ParametersDict:\n    \"\"\"Fit parameters of Gaussian process model by maximimizing posterior density.\n\n    Finds maximum-a-posterior (MAP) estimate of Gaussian process parameters by\n    minimizing negative logarithm of posterior density on parameters given data.\n\n    Args:\n        rng: Seeded NumPy random number generator.\n        gaussian_process: Tuple of functions for Gaussian process model to fit.\n        minimize_function: Function used to attempt to find global minimum of negative\n            log posterior density function.\n        **minimize_function_kwargs: Any keyword arguments to pass to\n            `minimize_function` function used to optimize negative posterior log density\n            function.\n\n    Returns:\n        Dictionary of parameters corresponding to maximum-a-posteriori estimate.\n    \"\"\"\n    if minimize_function is minimize_with_restarts:\n        minimize_function_kwargs.setdefault(\"number_minima_to_find\", 1)\n        minimize_function_kwargs.setdefault(\"maximum_minimize_calls\", 100)\n        minimize_function_kwargs.setdefault(\"minimize_method\", \"Newton-CG\")\n    unconstrained_parameters, _ = minimize_function(\n        objective_function=gaussian_process.neg_log_marginal_posterior,\n        sample_initial_state=lambda r: gaussian_process.sample_unconstrained_parameters(\n            r,\n            None,\n        ),\n        rng=rng,\n        **minimize_function_kwargs,\n    )\n    return gaussian_process.transform_parameters(unconstrained_parameters)\n</code></pre>"},{"location":"api/#calibr.emulation.get_gaussian_process_factory","title":"get_gaussian_process_factory","text":"<pre><code>get_gaussian_process_factory(\n    mean_function,\n    covariance_function,\n    neg_log_prior_density,\n    transform_parameters,\n    sample_unconstrained_parameters,\n)\n</code></pre> <p>Construct a factory function generating Gaussian process models given data.</p> <p>Parameters:</p> Name Type Description Default <code>mean_function</code> <code>MeanFunction</code> <p>Mean function for Gaussian process.</p> required <code>covariance_function</code> <code>CovarianceFunction</code> <p>Covariance function for Gaussian process.</p> required <code>neg_log_prior_density</code> <code>NegativeLogPriorDensity</code> <p>Negative logarithm of density of prior distribution on vector of unconstrained parameters for Gaussian process model.</p> required <code>transform_parameters</code> <code>ParameterTransformer</code> <p>Function which maps flat unconstrained parameter vector to a dictionary of (potential constrained) parameters, keyed by parameter name.</p> required <code>sample_unconstrained_parameters</code> <code>UnconstrainedParametersSampler</code> <p>Function generating random values for unconstrained vector of Gaussian process parameters.</p> required <p>Returns:</p> Type Description <code>GaussianProcessFactory</code> <p>Gaussian process factory function.</p> Source code in <code>src/calibr/emulation.py</code> <pre><code>def get_gaussian_process_factory(\n    mean_function: MeanFunction,\n    covariance_function: CovarianceFunction,\n    neg_log_prior_density: NegativeLogPriorDensity,\n    transform_parameters: ParameterTransformer,\n    sample_unconstrained_parameters: UnconstrainedParametersSampler,\n) -&gt; GaussianProcessFactory:\n    \"\"\"Construct a factory function generating Gaussian process models given data.\n\n    Args:\n        mean_function: Mean function for Gaussian process.\n        covariance_function: Covariance function for Gaussian process.\n        neg_log_prior_density: Negative logarithm of density of prior distribution on\n            vector of unconstrained parameters for Gaussian process model.\n        transform_parameters: Function which maps flat unconstrained parameter vector to\n            a dictionary of (potential constrained) parameters, keyed by parameter name.\n        sample_unconstrained_parameters: Function generating random values for\n            unconstrained vector of Gaussian process parameters.\n\n    Returns:\n        Gaussian process factory function.\n    \"\"\"\n\n    def gaussian_process_factory(data: DataDict) -&gt; GaussianProcessModel:\n        (\n            neg_log_marginal_likelihood,\n            get_posterior_functions,\n        ) = gaussian_process_with_isotropic_gaussian_observations(\n            data, mean_function, covariance_function\n        )\n\n        @jax.jit\n        def neg_log_marginal_posterior(unconstrained_parameters: ArrayLike) -&gt; float:\n            parameters = transform_parameters(unconstrained_parameters)\n            return neg_log_prior_density(\n                unconstrained_parameters\n            ) + neg_log_marginal_likelihood(parameters)\n\n        return GaussianProcessModel(\n            neg_log_marginal_posterior,\n            get_posterior_functions,\n            transform_parameters,\n            sample_unconstrained_parameters,\n        )\n\n    return gaussian_process_factory\n</code></pre>"},{"location":"api/#calibr.optimization","title":"optimization","text":"<p>Functions for minimization of objective functions.</p>"},{"location":"api/#calibr.optimization.ConvergenceError","title":"ConvergenceError","text":"<p>             Bases: <code>Exception</code></p> <p>Error raised when optimizer fails to converge within given computation budget.</p> Source code in <code>src/calibr/optimization.py</code> <pre><code>class ConvergenceError(Exception):\n    \"\"\"Error raised when optimizer fails to converge within given computation budget.\"\"\"\n</code></pre>"},{"location":"api/#calibr.optimization.GlobalMinimizer","title":"GlobalMinimizer","text":"<p>             Bases: <code>Protocol</code></p> <p>Function which attempts to find global minimum of a scalar objective function.</p> Source code in <code>src/calibr/optimization.py</code> <pre><code>class GlobalMinimizer(Protocol):\n    \"\"\"Function which attempts to find global minimum of a scalar objective function.\"\"\"\n\n    def __call__(\n        self,\n        objective_function: ObjectiveFunction,\n        sample_initial_state: InitialStateSampler,\n        rng: Generator,\n        **kwargs: GlobalMinimizerKwarg,\n    ) -&gt; tuple[jax.Array, float]:\n        \"\"\"\n        Minimize a differentiable objective function.\n\n        Args:\n            objective_function: Differentiable scalar-valued function of a single flat\n                vector argument to be minimized. Assumed to be specified using JAX\n                primitives such that its gradient and Hessian can be computed using\n                JAX's automatic differentiation support, and to be suitable for\n                just-in-time compilation.\n            sample_initial_state: Callable with one argument, which when passed a NumPy\n                random number generator returns a random initial state for optimization\n                of appropriate dimension.\n            rng: Seeded NumPy random number generator.\n            **kwargs: Any keyword arguments to global minimizer function.\n\n        Returns:\n            Tuple with first entry the state corresponding to the minima point and the\n            second entry the corresponding objective function value.\n        \"\"\"\n</code></pre>"},{"location":"api/#calibr.optimization.GlobalMinimizer.__call__","title":"__call__","text":"<pre><code>__call__(objective_function, sample_initial_state, rng, **kwargs)\n</code></pre> <p>Minimize a differentiable objective function.</p> <p>Parameters:</p> Name Type Description Default <code>objective_function</code> <code>ObjectiveFunction</code> <p>Differentiable scalar-valued function of a single flat vector argument to be minimized. Assumed to be specified using JAX primitives such that its gradient and Hessian can be computed using JAX's automatic differentiation support, and to be suitable for just-in-time compilation.</p> required <code>sample_initial_state</code> <code>InitialStateSampler</code> <p>Callable with one argument, which when passed a NumPy random number generator returns a random initial state for optimization of appropriate dimension.</p> required <code>rng</code> <code>Generator</code> <p>Seeded NumPy random number generator.</p> required <code>**kwargs</code> <code>GlobalMinimizerKwarg</code> <p>Any keyword arguments to global minimizer function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Tuple with first entry the state corresponding to the minima point and the</p> <code>float</code> <p>second entry the corresponding objective function value.</p> Source code in <code>src/calibr/optimization.py</code> <pre><code>def __call__(\n    self,\n    objective_function: ObjectiveFunction,\n    sample_initial_state: InitialStateSampler,\n    rng: Generator,\n    **kwargs: GlobalMinimizerKwarg,\n) -&gt; tuple[jax.Array, float]:\n    \"\"\"\n    Minimize a differentiable objective function.\n\n    Args:\n        objective_function: Differentiable scalar-valued function of a single flat\n            vector argument to be minimized. Assumed to be specified using JAX\n            primitives such that its gradient and Hessian can be computed using\n            JAX's automatic differentiation support, and to be suitable for\n            just-in-time compilation.\n        sample_initial_state: Callable with one argument, which when passed a NumPy\n            random number generator returns a random initial state for optimization\n            of appropriate dimension.\n        rng: Seeded NumPy random number generator.\n        **kwargs: Any keyword arguments to global minimizer function.\n\n    Returns:\n        Tuple with first entry the state corresponding to the minima point and the\n        second entry the corresponding objective function value.\n    \"\"\"\n</code></pre>"},{"location":"api/#calibr.optimization.basin_hopping","title":"basin_hopping","text":"<pre><code>basin_hopping(\n    objective_function,\n    sample_initial_state,\n    rng,\n    *,\n    num_iterations=5,\n    minimize_method=\"Newton-CG\",\n    minimize_max_iterations=None,\n    minimize_tol=None,\n    **unknown_kwargs\n)\n</code></pre> <p>Minimize a differentiable objective function with SciPy basin-hopping algorithm.</p> <p>The basin-hopping algorithm nests an inner local minimization using the <code>scipy.optimize.minimize</code> method within an outer global stepping algorithm which perturbs the current state with a random displacement and accepts or rejects this proposal using a Metropolis criterion.</p> <p>Parameters:</p> Name Type Description Default <code>objective_function</code> <code>ObjectiveFunction</code> <p>Differentiable scalar-valued function of a single flat vector argument to be minimized. Assumed to be specified using JAX primitives such that its gradient and Hessian can be computed using JAX's automatic differentiation support, and to be suitable for just-in-time compilation.</p> required <code>sample_initial_state</code> <code>InitialStateSampler</code> <p>Callable with one argument, which when passed a NumPy random number generator returns a random initial state for optimization of appropriate dimension.</p> required <code>rng</code> <code>Generator</code> <p>Seeded NumPy random number generator.</p> required <code>num_iterations</code> <code>int</code> <p>Number of basin-hopping iterations, with number of inner <code>scipy.optimize.minimize</code> calls being <code>num_iterations + 1</code>.</p> <code>5</code> <code>minimize_method</code> <code>str</code> <p>String specifying one of local optimization methods which can be passed to <code>method</code> argument of <code>scipy.optimize.minimize</code>.</p> <code>'Newton-CG'</code> <code>minimize_max_iterations</code> <code>int | None</code> <p>Maximum number of iterations in inner local minimization.</p> <code>None</code> <code>minimize_tol</code> <code>float | None</code> <p>Tolerance parameter for inner local minimization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Tuple with first entry the state corresponding to the best minima candidate</p> <code>float</code> <p>found and the second entry the corresponding objective function value.</p> Source code in <code>src/calibr/optimization.py</code> <pre><code>def basin_hopping(\n    objective_function: ObjectiveFunction,\n    sample_initial_state: InitialStateSampler,\n    rng: Generator,\n    *,\n    num_iterations: int = 5,\n    minimize_method: str = \"Newton-CG\",\n    minimize_max_iterations: int | None = None,\n    minimize_tol: float | None = None,\n    **unknown_kwargs: GlobalMinimizerKwarg,\n) -&gt; tuple[jax.Array, float]:\n    \"\"\"Minimize a differentiable objective function with SciPy basin-hopping algorithm.\n\n    The basin-hopping algorithm nests an inner local minimization using the\n    `scipy.optimize.minimize` method within an outer global stepping algorithm which\n    perturbs the current state with a random displacement and accepts or rejects this\n    proposal using a Metropolis criterion.\n\n    Args:\n        objective_function: Differentiable scalar-valued function of a single flat\n            vector argument to be minimized. Assumed to be specified using JAX\n            primitives such that its gradient and Hessian can be computed using JAX's\n            automatic differentiation support, and to be suitable for just-in-time\n            compilation.\n        sample_initial_state: Callable with one argument, which when passed a NumPy\n            random number generator returns a random initial state for optimization of\n            appropriate dimension.\n        rng: Seeded NumPy random number generator.\n        num_iterations: Number of basin-hopping iterations, with number of inner\n            `scipy.optimize.minimize` calls being `num_iterations + 1`.\n        minimize_method: String specifying one of local optimization methods which can\n            be passed to `method` argument of `scipy.optimize.minimize`.\n        minimize_max_iterations: Maximum number of iterations in inner local\n            minimization.\n        minimize_tol: Tolerance parameter for inner local minimization.\n\n    Returns:\n        Tuple with first entry the state corresponding to the best minima candidate\n        found and the second entry the corresponding objective function value.\n    \"\"\"\n    _check_unknown_kwargs(unknown_kwargs)\n    results = _basin_hopping(\n        jax.jit(objective_function),\n        x0=sample_initial_state(rng),\n        niter=num_iterations,\n        minimizer_kwargs={\n            \"method\": minimize_method,\n            \"jac\": jax.jit(jax.grad(objective_function)),\n            \"hessp\": jax.jit(hessian_vector_product(objective_function)),\n            \"tol\": minimize_tol,\n            \"options\": {\"maxiter\": minimize_max_iterations},\n        },\n        seed=rng,\n    )\n    return results.x, float(results.fun)\n</code></pre>"},{"location":"api/#calibr.optimization.hessian_vector_product","title":"hessian_vector_product","text":"<pre><code>hessian_vector_product(scalar_function)\n</code></pre> <p>Construct function to compute Hessian-vector product for scalar-valued function.</p> <p>Parameters:</p> Name Type Description Default <code>scalar_function</code> <code>ObjectiveFunction</code> <p>Scalar-valued objective function.</p> required <p>Returns:</p> Type Description <code>Callable[[ArrayLike, ArrayLike], Array]</code> <p>Hessian-vector product function.</p> Source code in <code>src/calibr/optimization.py</code> <pre><code>def hessian_vector_product(\n    scalar_function: ObjectiveFunction,\n) -&gt; Callable[[ArrayLike, ArrayLike], jax.Array]:\n    \"\"\"\n    Construct function to compute Hessian-vector product for scalar-valued function.\n\n    Args:\n        scalar_function: Scalar-valued objective function.\n\n    Returns:\n        Hessian-vector product function.\n    \"\"\"\n\n    def hvp(x: ArrayLike, v: ArrayLike) -&gt; jax.Array:\n        return jax.jvp(jax.grad(scalar_function), (x,), (v,))[1]\n\n    return hvp\n</code></pre>"},{"location":"api/#calibr.optimization.minimize_with_restarts","title":"minimize_with_restarts","text":"<pre><code>minimize_with_restarts(\n    objective_function,\n    sample_initial_state,\n    rng,\n    *,\n    number_minima_to_find=5,\n    maximum_minimize_calls=100,\n    minimize_method=\"Newton-CG\",\n    minimize_max_iterations=None,\n    minimize_tol=None,\n    logging_function=lambda _: None,\n    **unknown_kwargs\n)\n</code></pre> <p>Minimize a differentiable objective function with random restarts.</p> <p>Iteratively calls <code>scipy.optimize.minimize</code> to attempt to find a minimum of an objective function until a specified number of candidate minima are successfully found, with the initial state for each <code>minimize</code> called being randomly sampled using a user provided function. The candidate minima with the minimum value for the objective function is returned along with the corresponding objective function value.</p> <p>Parameters:</p> Name Type Description Default <code>objective_function</code> <code>ObjectiveFunction</code> <p>Differentiable scalar-valued function of a single flat vector argument to be minimized. Assumed to be specified using JAX primitives such that its gradient and Hessian can be computed using JAX's automatic differentiation support, and to be suitable for just-in-time compilation.</p> required <code>sample_initial_state</code> <code>InitialStateSampler</code> <p>Callable with one argument, which when passed a NumPy random number generator returns a random initial state for optimization of appropriate dimension.</p> required <code>rng</code> <code>Generator</code> <p>Seeded NumPy random number generator.</p> required <p>Other Parameters:</p> Name Type Description <code>number_minima_to_find</code> <code>int</code> <p>Number of candidate minima of objective function to try to find.</p> <code>maximum_minimize_calls</code> <code>int</code> <p>Maximum number of times to try calling <code>scipy.optimize.minimize</code> to find candidate minima. If insufficient candidates are found within this number of calls then a <code>ConvergenceError</code> exception is raised.</p> <code>minimize_method</code> <code>str</code> <p>String specifying one of local optimization methods which can be passed to <code>method</code> argument of <code>scipy.optimize.minimize</code>.</p> <code>minimize_max_iterations</code> <code>int | None</code> <p>Maximum number of iterations in inner local minimization.</p> <code>minimize_tol</code> <code>float | None</code> <p>Tolerance parameter for inner local minimization.</p> <code>logging_function</code> <code>Callable[[str], None]</code> <p>Function to use to optionally log status messages during minimization. Defaults to a no-op function which discards messages.</p> <p>Returns:</p> Type Description <code>Array</code> <p>Tuple with first entry the state corresponding to the best minima candidate</p> <code>float</code> <p>found and the second entry the corresponding objective function value.</p> Source code in <code>src/calibr/optimization.py</code> <pre><code>def minimize_with_restarts(\n    objective_function: ObjectiveFunction,\n    sample_initial_state: InitialStateSampler,\n    rng: Generator,\n    *,\n    number_minima_to_find: int = 5,\n    maximum_minimize_calls: int = 100,\n    minimize_method: str = \"Newton-CG\",\n    minimize_max_iterations: int | None = None,\n    minimize_tol: float | None = None,\n    logging_function: Callable[[str], None] = lambda _: None,\n    **unknown_kwargs: GlobalMinimizerKwarg,\n) -&gt; tuple[jax.Array, float]:\n    \"\"\"Minimize a differentiable objective function with random restarts.\n\n    Iteratively calls `scipy.optimize.minimize` to attempt to find a minimum of an\n    objective function until a specified number of candidate minima are successfully\n    found, with the initial state for each `minimize` called being randomly sampled\n    using a user provided function. The candidate minima with the minimum value for\n    the objective function is returned along with the corresponding objective function\n    value.\n\n    Args:\n        objective_function: Differentiable scalar-valued function of a single flat\n            vector argument to be minimized. Assumed to be specified using JAX\n            primitives such that its gradient and Hessian can be computed using JAX's\n            automatic differentiation support, and to be suitable for just-in-time\n            compilation.\n        sample_initial_state: Callable with one argument, which when passed a NumPy\n            random number generator returns a random initial state for optimization of\n            appropriate dimension.\n        rng: Seeded NumPy random number generator.\n\n    Keyword Args:\n        number_minima_to_find: Number of candidate minima of objective function to try\n            to find.\n        maximum_minimize_calls: Maximum number of times to try calling\n            `scipy.optimize.minimize` to find candidate minima. If insufficient\n            candidates are found within this number of calls then a `ConvergenceError`\n            exception is raised.\n        minimize_method: String specifying one of local optimization methods which can\n            be passed to `method` argument of `scipy.optimize.minimize`.\n        minimize_max_iterations: Maximum number of iterations in inner local\n            minimization.\n        minimize_tol: Tolerance parameter for inner local minimization.\n        logging_function: Function to use to optionally log status messages during\n            minimization. Defaults to a no-op function which discards messages.\n\n    Returns:\n        Tuple with first entry the state corresponding to the best minima candidate\n        found and the second entry the corresponding objective function value.\n    \"\"\"\n    _check_unknown_kwargs(unknown_kwargs)\n    minima_found: list[tuple[jax.Array, int, jax.Array]] = []\n    minimize_calls = 0\n    while (\n        len(minima_found) &lt; number_minima_to_find\n        and minimize_calls &lt; maximum_minimize_calls\n    ):\n        logging_function(f\"Starting minimize call {minimize_calls + 1}\")\n        results = _minimize(\n            jax.jit(objective_function),\n            x0=sample_initial_state(rng),\n            jac=jax.jit(jax.grad(objective_function)),\n            hessp=jax.jit(hessian_vector_product(objective_function)),\n            method=minimize_method,\n            tol=minimize_tol,\n            options={\"maxiter\": minimize_max_iterations},\n        )\n        minimize_calls += 1\n        if results.success:\n            logging_function(f\"Found minima with value {results.fun}\")\n            # Add minima to minima_found maintaining heap invariant such that first\n            # entry in minima_found is always best solution so far (with counter used\n            # to break ties between solutions with equal values for objective)\n            heappush(minima_found, (results.fun, len(minima_found), results.x))\n        else:\n            logging_function(f\"Minimization unsuccessful - {results.message}\")\n    if len(minima_found) &lt; number_minima_to_find:\n        msg = (\n            f\"Did not find required {number_minima_to_find} minima in \"\n            f\"{maximum_minimize_calls} minimize calls.\"\n        )\n        raise ConvergenceError(msg)\n    # Heap property means first entry in minima_found will correspond to solution\n    # with minimum acquisition function value\n    (\n        min_objective_function,\n        _,\n        state,\n    ) = minima_found[0]\n    logging_function(f\"Best minima found has value {min_objective_function}\")\n    return state, float(min_objective_function)\n</code></pre>"}]}